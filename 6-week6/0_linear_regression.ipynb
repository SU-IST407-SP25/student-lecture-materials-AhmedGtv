{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Linear Regression and Gradient Descent\n",
    "\n",
    "## Linear Regression: The Basics\n",
    "\n",
    "Linear regression is a fundamental technique in machine learning used to model the relationship between one or more independent variables (features) and a dependent variable (target). At its core, linear regression attempts to fit a linear equation to observed data.\n",
    "\n",
    "### Simple Linear Regression\n",
    "\n",
    "Let's start with the simplest case: one feature and one target variable. The equation for simple linear regression is:\n",
    "\n",
    "$$ y = mx + b $$\n",
    "\n",
    "Where:\n",
    "- y is the predicted value\n",
    "- x is the feature value\n",
    "- m is the slope (also called the coefficient or weight)\n",
    "- b is the y-intercept (also called the bias)\n",
    "\n",
    "Visually, this looks like fitting a straight line to a set of data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate some sample data\n",
    "np.random.seed(0)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = 2 * X + 1 + np.random.randn(100, 1) * 1\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, 2*X + 1, color='red')\n",
    "plt.title('Simple Linear Regression')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Multiple Linear Regression\n",
    "\n",
    "When we have multiple features, we extend this concept to multiple dimensions. The equation becomes:\n",
    "\n",
    "$$ y = w_1x_1 + w_2x_2 + ... + w_nx_n + b $$\n",
    "\n",
    "Where:\n",
    "- y is the predicted value\n",
    "- $x_1, x_2, ..., x_n$  are the feature values\n",
    "- $w_1, w_2, ..., w_n$ are the weights for each feature\n",
    "- $b$ is the bias term\n",
    "\n",
    "This is essentially the same as our simple linear regression, but now we're working in a higher-dimensional space. Instead of fitting a line, we're fitting a hyperplane.\n",
    "\n",
    "It's important to understand that the core concept remains the same whether we're working with one feature or many. We're still trying to find the best linear relationship between our features and the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Gradient Descent: Optimizing Our Model\n",
    "\n",
    "Now that we understand what linear regression is trying to do, how do we actually find the best values for our weights and bias? One of the most common approaches, used throughout machine learning, is called *gradient descent*.  \n",
    "\n",
    "## What is a Gradient?\n",
    "\n",
    "A gradient is a generalization of the concept of a derivative to functions of multiple variables. In the context of machine learning and optimization, the gradient represents the direction and rate of fastest increase of a function at a particular point.\n",
    "\n",
    "### From Slopes to Derivatives to Gradients\n",
    "\n",
    "1. **Slope**: In its simplest form, a slope is the steepness of a line. It's calculated as the change in $y$ divided by the change in $x$: $(y_2 - y_1) / (x_2 - x_1)$.\n",
    "\n",
    "2. **Derivative**: A derivative is the instantaneous rate of change of a function at any given point. It's the limit of the slope as the distance between two points approaches zero. For a function $f(x)$, the derivative is written as $f'(x)$ or $df/dx$.\n",
    "\n",
    "3. **Gradient**: The gradient is the multi-dimensional extension of the derivative. For a function of multiple variables,  $f(x_1, x_2, ..., x_n)$, the gradient is a vector of partial derivatives with respect to each variable:\n",
    "\n",
    "   $$\\nabla f = \\left[\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, ..., \\frac{\\partial f}{\\partial x_n}\\right]$$\n",
    "\n",
    "   Where $\\nabla$ is the gradient operator, and $\\frac{\\partial f}{\\partial x_1}$ is the partial derivative of $f$ with respect to $x_1$.\n",
    "\n",
    "In the context of linear regression, each component of the gradient represents how much the loss function would change if we slightly adjusted the corresponding weight or bias.\n",
    "\n",
    "## Determining Gradients\n",
    "\n",
    "Gradients are determined by calculating the partial derivatives of the function with respect to each of its variables. In practice, this often involves:\n",
    "\n",
    "1. **Analytical derivation**: Using calculus rules to derive the gradient formula.\n",
    "2. **Numerical approximation**: Using finite differences to estimate the gradient.\n",
    "3. **Automatic differentiation**: Leveraging computational graphs to automatically compute gradients (common in deep learning frameworks).\n",
    "\n",
    "For linear regression, we can analytically derive the gradients. Let's consider the Mean Squared Error (MSE) loss function:\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Where $n$ is the number of samples, $y_i$ is the true value, and $\\hat{y}_i$ is the predicted value.\n",
    "\n",
    "For a simple linear regression ($\\hat{y} = wx + b$), the gradients are:\n",
    "\n",
    "$$\\frac{\\partial \\text{MSE}}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i) \\cdot x_i$$\n",
    "$$\\frac{\\partial \\text{MSE}}{\\partial b} = -\\frac{2}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)$$\n",
    "\n",
    "## What is a Loss Function?\n",
    "\n",
    "A loss function, also known as a cost function or objective function, measures how well our model's predictions match the actual data. It quantifies the \"error\" or \"loss\" associated with our current model parameters.\n",
    "\n",
    "Key points about loss functions:\n",
    "\n",
    "1. **Purpose**: They provide a single number representing the model's performance, which we aim to minimize.\n",
    "2. **Choice**: Different problems may require different loss functions. Common choices include:\n",
    "   - Mean Squared Error (MSE) for regression\n",
    "   - Cross-entropy for classification\n",
    "3. **Optimization**: Machine learning algorithms often work by minimizing the loss function through techniques like gradient descent.\n",
    "\n",
    "For linear regression, we commonly use the Mean Squared Error (MSE) as our loss function. MSE has several desirable properties:\n",
    "- It's always non-negative (squared terms)\n",
    "- It penalizes larger errors more heavily (quadratic)\n",
    "- It's differentiable, allowing us to compute gradients\n",
    "\n",
    "## Putting it All Together: Gradient Descent\n",
    "\n",
    "Gradient descent uses the gradients of the loss function to iteratively adjust the model parameters (weights and bias in linear regression) to minimize the loss. The process works as follows:\n",
    "\n",
    "1. Start with initial parameter values.\n",
    "2. Compute the gradient of the loss function with respect to each parameter.\n",
    "3. Update each parameter by subtracting a small step in the direction of its gradient:\n",
    "    $$w=w-\\text{learning\\_rate}\\cdot\\frac{\\partial\\text{MSE}}{\\partial w}$$\n",
    "    $$b=b-\\text{learning\\_rate}\\cdot\\frac{\\partial\\text{MSE}}{\\partial b}$$\n",
    "4. Repeat steps 2-3 until convergence or for a fixed number of iterations.\n",
    "\n",
    "The learning rate determines the size of the steps we take. Too large, and we might overshoot the minimum; too small, and convergence will be slow.\n",
    "\n",
    "Let's visualize this process for a simple 1D optimization problem:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return x**2 + 5*np.sin(x)\n",
    "\n",
    "def df(x):\n",
    "    return 2*x + 5*np.cos(x)\n",
    "\n",
    "x = np.linspace(-10, 10, 200)\n",
    "y = f(x)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, y)\n",
    "plt.title('Optimization Landscape')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "\n",
    "# Gradient descent\n",
    "x_current = 8\n",
    "learning_rate = 0.1\n",
    "n_iterations = 50\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    plt.scatter(x_current, f(x_current), color='red')\n",
    "    x_current = x_current - learning_rate * df(x_current)\n",
    "\n",
    "plt.scatter(x_current, f(x_current), color='green', s=100, label='Final position')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### The Process of Gradient Descent\n",
    "\n",
    "1. Start with some initial values for the weights and bias.\n",
    "2. Calculate the predictions using these current parameter values.\n",
    "3. Compute the cost (error) between our predictions and the actual target values.\n",
    "4. Calculate the gradient of the cost with respect to each parameter.\n",
    "5. Update each parameter by subtracting a small portion of its gradient.\n",
    "6. Repeat steps 2-5 until the cost converges or we reach a maximum number of iterations.\n",
    "\n",
    "The \"small portion\" mentioned in step 5 is determined by the learning rate. A higher learning rate means larger steps, potentially reaching the minimum faster but risking overshooting. A lower learning rate means smaller, more careful steps, but may take longer to converge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Non-convex cost functions\n",
    "\n",
    "Note that for more complex problems, the optimization surface may be \"non-convex,\" which means that it might have several local optima, which means that gradient descent can get trapped.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return x**4 - 4*x**2 + 5*np.sin(x)\n",
    "\n",
    "def df(x):\n",
    "    return 4*x**3 - 8*x + 5*np.cos(x)\n",
    "\n",
    "x = np.linspace(-3, 3, 300)\n",
    "y = f(x)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, y)\n",
    "plt.title('Non-Convex Optimization Landscape')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "\n",
    "# Gradient descent from multiple starting points\n",
    "learning_rate = 0.05\n",
    "n_iterations = 50\n",
    "starting_points = [-2.5, -1, 0, 1, 2.5]\n",
    "colors = ['red', 'green', 'blue', 'purple', 'orange']\n",
    "all_trajectories = []\n",
    "\n",
    "for start, color in zip(starting_points, colors):\n",
    "    x_current = start\n",
    "    trajectory = [x_current]\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        x_current = x_current - learning_rate * df(x_current)\n",
    "        trajectory.append(x_current)\n",
    "    \n",
    "    trajectory = np.array(trajectory)\n",
    "    all_trajectories.append(trajectory)\n",
    "    plt.scatter(trajectory, f(trajectory), color=color, s=30, alpha=0.5, label=f'Start: {start}')\n",
    "    plt.plot(trajectory, f(trajectory), color=color, alpha=0.3)\n",
    "    plt.scatter(trajectory[-1], f(trajectory[-1]), color=color, s=100, edgecolor='black')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print final positions\n",
    "idx = 0\n",
    "for start, color in zip(starting_points, colors):\n",
    "    final_x = all_trajectories[idx][-1]\n",
    "    idx+=1\n",
    "    print(f\"Starting at {start:.2f}, ended at {final_x:.2f} with value {f(final_x):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding example illustrates several different subtle problems.\n",
    "\n",
    "1. Multiple Local Minima:\n",
    "   The function we've chosen has multiple local minima. This is a common challenge in complex optimization problems, including many machine learning scenarios. In our example, depending on the starting point, gradient descent may converge to different local minima.\n",
    "\n",
    "2. Sensitivity to Initial Conditions:\n",
    "   The final result of the optimization process is highly dependent on the starting point. This highlights the importance of initialization in machine learning models, especially in deep learning where good initialization strategies can significantly impact model performance.\n",
    "\n",
    "3. Potential for Getting Stuck:\n",
    "   In some cases, the algorithm might get stuck in a local minimum that is not the global minimum. This is why more advanced optimization techniques often incorporate mechanisms to escape local minima.\n",
    "\n",
    "4. Plateaus and Saddle Points:\n",
    "   While not prominently featured in this specific function, many non-convex functions in higher dimensions have plateaus or saddle points where the gradient is close to zero. This can slow down or stall the optimization process.\n",
    "\n",
    "5. Learning Rate Sensitivity:\n",
    "   The choice of learning rate becomes more critical in non-convex scenarios. A large learning rate might cause the algorithm to overshoot and miss minima, while a small learning rate might result in slow convergence or getting stuck in poor local minima.\n",
    "\n",
    "6. Need for Multiple Runs:\n",
    "   In practice, when dealing with non-convex optimization problems, it's often necessary to run the optimization multiple times from different starting points to increase the chances of finding a good solution.\n",
    "\n",
    "7. Difficulty in Assessing Global Optimality:\n",
    "   Unlike convex optimization, where reaching a local minimum guarantees global optimality, in non-convex optimization, it's generally very difficult to determine if a found solution is globally optimal.\n",
    "\n",
    "Implications for Machine Learning:\n",
    "- In deep learning, the loss landscapes are typically highly non-convex. This is one reason why training deep neural networks can be challenging and why techniques like stochastic gradient descent, adaptive learning rates, and momentum have been developed.\n",
    "- Ensemble methods in machine learning can be seen as a way to mitigate the risks associated with non-convex optimization by combining multiple models, each potentially converging to different local optima.\n",
    "- Techniques like simulated annealing, genetic algorithms, and more advanced variants of gradient descent (e.g., Adam, RMSprop) are often employed to better navigate non-convex landscapes.\n",
    "\n",
    "This example and discussion highlight why optimization in machine learning is often more complex than simple gradient descent on convex functions, and why ongoing research in optimization algorithms remains crucial for advancing the field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Simple Linear Regression\n",
    "\n",
    "Linear regression for a simple, one dimensional problem is called Simple Linear Regression.  The heart of this is just the gradient descent. The following code illustrates how this can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate some sample data\n",
    "np.random.seed(0)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = 2 * X + 1 + np.random.randn(100, 1) * 0.5\n",
    "\n",
    "# Initialize parameters\n",
    "w = 0\n",
    "b = 0\n",
    "learning_rate = 0.01\n",
    "n_iterations = 1000\n",
    "\n",
    "# Lists to store the parameter and MSE history\n",
    "w_history = [w]\n",
    "b_history = [b]\n",
    "mse_history = []\n",
    "\n",
    "# Gradient descent\n",
    "for i in range(n_iterations):\n",
    "    # Make predictions\n",
    "\n",
    "    y_pred = w * X + b\n",
    "    \n",
    "    # Compute MSE\n",
    "    mse = np.mean((y_pred - y) ** 2)\n",
    "    mse_history.append(mse)\n",
    "    \n",
    "    # Compute gradients\n",
    "    dw = (2 / len(X)) * np.sum((y_pred - y) * X)\n",
    "    db = (2 / len(X)) * np.sum(y_pred - y)\n",
    "    \n",
    "    # Update parameters\n",
    "    w -= learning_rate * dw\n",
    "    b -= learning_rate * db\n",
    "    \n",
    "    # Store parameter history\n",
    "    w_history.append(w)\n",
    "    b_history.append(b)\n",
    "    \n",
    "    # Print progress every 100 iterations\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iteration {i}: MSE = {mse:.4f}, w = {w:.4f}, b = {b:.4f}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot the data and the final regression line\n",
    "plt.subplot(121)\n",
    "plt.scatter(X, y, alpha=0.5)\n",
    "plt.plot(X, w*X + b, color='red')\n",
    "plt.title('Linear Regression with Gradient Descent')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Plot the MSE history\n",
    "plt.subplot(122)\n",
    "plt.plot(mse_history)\n",
    "plt.title('Mean Squared Error vs. Iterations')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('MSE')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final parameters: w = {w:.4f}, b = {b:.4f}\")\n",
    "\n",
    "# Plot parameter convergence\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(w_history)\n",
    "plt.title('Weight (w) Convergence')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('w')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(b_history)\n",
    "plt.title('Bias (b) Convergence')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('b')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Simple to Multiple Linear Regression: A Matrix Approach\n",
    "\n",
    "## Simple Linear Regression\n",
    "\n",
    "In simple linear regression, we have one independent variable $x$ and one dependent variable $y$. The model is represented as:\n",
    "\n",
    "$$y = wx + b$$\n",
    "\n",
    "Where $w$ is the weight (or slope) and $b$ is the bias (or intercept).\n",
    "\n",
    "For $n$ data points, we can write this as a system of equations:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y_1 &= wx_1 + b \\\\\n",
    "y_2 &= wx_2 + b \\\\\n",
    "&\\vdots \\\\\n",
    "y_n &= wx_n + b\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## Multiple Linear Regression\n",
    "\n",
    "In multiple linear regression, we have multiple independent variables $x_1, x_2, ..., x_m$ and one dependent variable $y$. The model is represented as:\n",
    "\n",
    "$$y = w_1x_1 + w_2x_2 + ... + w_mx_m + b$$\n",
    "\n",
    "For $n$ data points and $m$ features, we can write this as a system of equations:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y_1 &= w_1x_{11} + w_2x_{12} + ... + w_mx_{1m} + b \\\\\n",
    "y_2 &= w_1x_{21} + w_2x_{22} + ... + w_mx_{2m} + b \\\\\n",
    "&\\vdots \\\\\n",
    "y_n &= w_1x_{n1} + w_2x_{n2} + ... + w_mx_{nm} + b\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## Matrix Representation\n",
    "\n",
    "We can represent this system of equations using matrices:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{X}\\mathbf{w} + \\mathbf{b}$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{y}$ is an $n \\times 1$ vector of dependent variables\n",
    "- $\\mathbf{X}$ is an $n \\times (m+1)$ matrix of independent variables (including a column of 1s for the bias term)\n",
    "- $\\mathbf{w}$ is an $(m+1) \\times 1$ vector of weights (including the bias as the last element)\n",
    "\n",
    "In expanded form:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1m} & 1 \\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2m} & 1 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "x_{n1} & x_{n2} & \\cdots & x_{nm} & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_m \\\\\n",
    "b\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Understanding np.dot\n",
    "\n",
    "The NumPy function `np.dot` performs matrix multiplication. When we use `np.dot(X, w)`, we're essentially computing the sum of element-wise multiplications for each row of $\\mathbf{X}$ with $\\mathbf{w}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y_1 &= x_{11}w_1 + x_{12}w_2 + ... + x_{1m}w_m + 1 \\cdot b \\\\\n",
    "y_2 &= x_{21}w_1 + x_{22}w_2 + ... + x_{2m}w_m + 1 \\cdot b \\\\\n",
    "&\\vdots \\\\\n",
    "y_n &= x_{n1}w_1 + x_{n2}w_2 + ... + x_{nm}w_m + 1 \\cdot b\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This operation efficiently computes the predictions for all data points simultaneously.\n",
    "\n",
    "## Gradient Computation\n",
    "\n",
    "The gradient of the Mean Squared Error (MSE) loss function with respect to the weights can also be computed using matrix operations:\n",
    "\n",
    "$$\\nabla_w \\text{MSE} = \\frac{2}{n} \\mathbf{X}^T (\\mathbf{X}\\mathbf{w} - \\mathbf{y})$$\n",
    "\n",
    "Where $\\mathbf{X}^T$ is the transpose of $\\mathbf{X}$.\n",
    "\n",
    "This matrix operation efficiently computes the gradients for all weights simultaneously, allowing for vectorized implementations of gradient descent.\n",
    "\n",
    "## Practical Implementation\n",
    "\n",
    "In practice, we can implement this in NumPy as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "n_samples = 100\n",
    "n_features = 2\n",
    "\n",
    "# Create feature matrix X with two features\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Create target vector y\n",
    "true_weights = [2, -3.5]\n",
    "true_bias = 5\n",
    "y = np.dot(X, true_weights) + true_bias + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "# Add a column of 1s to X for the bias term\n",
    "# np.column_stack concatenates arrays along the second axis (columns)\n",
    "# np.ones(X.shape[0]) creates an array of 1s with the same number of rows as X\n",
    "X_with_bias = np.column_stack([X, np.ones(X.shape[0])])\n",
    "\n",
    "# Initialize weights (including bias) with zeros\n",
    "# The shape is X_with_bias.shape[1] to match the number of features plus the bias term\n",
    "w = np.zeros(X_with_bias.shape[1])\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_iterations = 1000\n",
    "\n",
    "# Lists to store the loss history\n",
    "loss_history = []\n",
    "\n",
    "# Gradient descent\n",
    "for i in range(n_iterations):\n",
    "    # Compute predictions using matrix multiplication\n",
    "    y_pred = np.dot(X_with_bias, w)\n",
    "    \n",
    "    # Compute the loss (Mean Squared Error)\n",
    "    loss = np.mean((y_pred - y) ** 2)\n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    # Compute gradients using matrix operations\n",
    "    # The transpose of X_with_bias (X_with_bias.T) is used to match dimensions for matrix multiplication\n",
    "    gradients = (2 / n_samples) * np.dot(X_with_bias.T, (y_pred - y))\n",
    "    \n",
    "    # Update weights using gradient descent\n",
    "    w -= learning_rate * gradients\n",
    "\n",
    "    # Print progress every 100 iterations\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iteration {i}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Print the final weights and bias\n",
    "print(\"Final weights:\", w[:-1])\n",
    "print(\"Final bias:\", w[-1])\n",
    "\n",
    "# Visualize the loss history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_history)\n",
    "plt.title('Loss vs. Iterations')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()\n",
    "\n",
    "# For 2D data, we can visualize the regression plane\n",
    "if n_features == 2:\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot the original data points\n",
    "    ax.scatter(X[:, 0], X[:, 1], y, c='b', marker='o', alpha=0.5)\n",
    "    \n",
    "    # Create a meshgrid for the regression plane\n",
    "    x0_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 50)\n",
    "    x1_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 50)\n",
    "    X0, X1 = np.meshgrid(x0_range, x1_range)\n",
    "    \n",
    "    # Compute the predicted values for the meshgrid\n",
    "    Z = w[0] * X0 + w[1] * X1 + w[2]\n",
    "    \n",
    "    # Plot the regression plane\n",
    "    ax.plot_surface(X0, X1, Z, alpha=0.3, cmap='viridis')\n",
    "    \n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_zlabel('Target')\n",
    "    ax.set_title('Linear Regression in 3D')\n",
    "\n",
    "    elev = 10   # elevation angle in degrees\n",
    "    azim = 45   # azimuth angle in degrees\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
