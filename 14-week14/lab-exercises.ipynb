{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d5d1dfa",
   "metadata": {},
   "source": [
    "# Exercise 1: Implementing K-means Clustering\n",
    "\n",
    "In this exercise, you'll implement the core components of the k-means clustering algorithm. Most of the setup code is provided - you just need to implement the key calculations for cluster assignment and centroid updates.\n",
    "\n",
    "## Setup\n",
    "First, let's import our required libraries and create some sample data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcb7544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50)\n",
    "plt.title(\"Generated Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08104ab",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 1: Initialize Centroids\n",
    "The initialization code is provided. This randomly selects k points as initial centroids:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18941b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_centroids(X, k):\n",
    "    n_samples = X.shape[0]\n",
    "    random_indices = np.random.choice(n_samples, k, replace=False)\n",
    "    centroids = X[random_indices]\n",
    "    return centroids\n",
    "\n",
    "# Initialize with k=4 clusters\n",
    "k = 4\n",
    "centroids = initialize_centroids(X, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed504e88",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 2: Implement Cluster Assignment\n",
    "Fill in the missing code in the `assign_clusters` function. This function should:\n",
    "1. Calculate the distance between each point and each centroid\n",
    "2. Assign each point to the nearest centroid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14466b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters(X, centroids):\n",
    "    # Calculate distances between each point and each centroid\n",
    "    # Hint: Use broadcasting and np.sum with axis=1\n",
    "    distances = # YOUR CODE HERE\n",
    "    \n",
    "    # Find the closest centroid for each point\n",
    "    # Hint: Use np.argmin with axis=1\n",
    "    labels = # YOUR CODE HERE\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c842c3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 3: Implement Centroid Updates\n",
    "Fill in the missing code in the `update_centroids` function. This function should:\n",
    "1. Calculate the mean position of all points assigned to each cluster\n",
    "2. Return these means as the new centroids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179587c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_centroids(X, labels, k):\n",
    "    centroids = np.zeros((k, X.shape[1]))\n",
    "    for i in range(k):\n",
    "        # Calculate mean position of points in cluster i\n",
    "        # Hint: Use boolean indexing with labels == i\n",
    "        centroids[i] = # YOUR CODE HERE\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7f7a9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 4: Run K-means\n",
    "The main k-means loop is provided. This will use your functions to run the algorithm:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d986a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(X, k, max_iters=100):\n",
    "    # Initialize centroids\n",
    "    centroids = initialize_centroids(X, k)\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        # Assign clusters\n",
    "        old_labels = assign_clusters(X, centroids)\n",
    "        \n",
    "        # Update centroids\n",
    "        centroids = update_centroids(X, old_labels, k)\n",
    "        \n",
    "        # Check for convergence\n",
    "        new_labels = assign_clusters(X, centroids)\n",
    "        if np.all(old_labels == new_labels):\n",
    "            break\n",
    "            \n",
    "    return centroids, new_labels\n",
    "\n",
    "# Run kmeans\n",
    "final_centroids, labels = kmeans(X, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c44d49",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 5: Visualize Results\n",
    "Use this code to visualize your clustering results:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fdfedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Plot original data\n",
    "plt.subplot(121)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50)\n",
    "plt.title(\"Original Data\")\n",
    "\n",
    "# Plot clustered data\n",
    "plt.subplot(122)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
    "plt.scatter(final_centroids[:, 0], final_centroids[:, 1], \n",
    "            c='red', marker='x', s=200, linewidths=3)\n",
    "plt.title(\"Clustered Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d9aed7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Discussion Questions\n",
    "1. How well did your implementation work? Are the clusters what you expected?\n",
    "2. What happens if you change the number of clusters (k)?\n",
    "3. Try changing the cluster_std parameter in make_blobs. How does this affect the clustering?\n",
    "\n",
    "\n",
    "# Exercise 2: Finding the Optimal Number of Clusters\n",
    "\n",
    "In this exercise, you'll implement and compare different methods for finding the optimal number of clusters: the elbow method and silhouette analysis. You'll work with a dataset where the true number of clusters isn't immediately obvious.\n",
    "\n",
    "## Setup\n",
    "First, let's import our libraries and create some sample data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50bdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate sample data with overlapping clusters\n",
    "np.random.seed(42)\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=[1.0, 2.0, 1.5, 1.8], random_state=42)\n",
    "\n",
    "# Add some random noise to make it more challenging\n",
    "X += np.random.normal(0, 0.1, X.shape)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50)\n",
    "plt.title(\"Generated Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb40b5b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 1: Implement the Elbow Method\n",
    "Complete the function to calculate inertia (within-cluster sum of squares) for different values of k:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac29649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_inertia(X, k_range):\n",
    "    inertias = []\n",
    "    \n",
    "    for k in k_range:\n",
    "        # Create and fit KMeans model\n",
    "        # Hint: Use sklearn.cluster.KMeans\n",
    "        kmeans = # YOUR CODE HERE\n",
    "        \n",
    "        # Get the inertia value\n",
    "        # Hint: Access the inertia_ attribute\n",
    "        inertia = # YOUR CODE HERE\n",
    "        \n",
    "        inertias.append(inertia)\n",
    "    \n",
    "    return inertias\n",
    "\n",
    "# Calculate inertia for k=1 to 10\n",
    "k_range = range(1, 11)\n",
    "inertias = calculate_inertia(X, k_range)\n",
    "\n",
    "# Plotting code is provided\n",
    "plt.plot(k_range, inertias, 'bo-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e593b7f3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 2: Implement Silhouette Analysis\n",
    "Complete the function to calculate silhouette scores for different values of k:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_silhouette(X, k_range):\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    for k in k_range:\n",
    "        if k <= 1:  # Skip k=1 as silhouette score isn't defined for 1 cluster\n",
    "            silhouette_scores.append(0)\n",
    "            continue\n",
    "            \n",
    "        # Create and fit KMeans model\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        # Calculate silhouette score\n",
    "        # Hint: Use sklearn.metrics.silhouette_score\n",
    "        score = # YOUR CODE HERE\n",
    "        \n",
    "        silhouette_scores.append(score)\n",
    "    \n",
    "    return silhouette_scores\n",
    "\n",
    "# Calculate silhouette scores for k=1 to 10\n",
    "silhouette_scores = calculate_silhouette(X, k_range)\n",
    "\n",
    "# Plotting code is provided\n",
    "plt.plot(k_range, silhouette_scores, 'ro-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03526a7d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 3: Visualize Clusters with Different k\n",
    "Use this code to visualize clustering results with different k values:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131be397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(X, k_values):\n",
    "    fig, axes = plt.subplots(1, len(k_values), figsize=(5*len(k_values), 4))\n",
    "    \n",
    "    for ax, k in zip(axes, k_values):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)\n",
    "        ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
    "                  c='red', marker='x', s=200, linewidths=3)\n",
    "        ax.set_title(f'k={k}')\n",
    "\n",
    "# Try with different k values\n",
    "plot_clusters(X, [2, 3, 4, 5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ef981",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 4: Compare Methods\n",
    "Complete this function to suggest the optimal k value based on both methods:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8773f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_k(inertias, silhouette_scores, k_range):\n",
    "    # Calculate the elbow point\n",
    "    # Hint: Look for the point of maximum curvature in inertias\n",
    "    # One simple way: Find where the rate of decrease slows down significantly\n",
    "    inertia_differences = np.diff(inertias)\n",
    "    elbow_k = # YOUR CODE HERE\n",
    "    \n",
    "    # Find k with highest silhouette score\n",
    "    # Hint: Use np.argmax\n",
    "    silhouette_k = # YOUR CODE HERE\n",
    "    \n",
    "    return elbow_k, silhouette_k\n",
    "\n",
    "elbow_k, silhouette_k = suggest_k(inertias, silhouette_scores, k_range)\n",
    "print(f\"Elbow method suggests k={elbow_k}\")\n",
    "print(f\"Silhouette analysis suggests k={silhouette_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034775e7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Discussion Questions\n",
    "1. Do the elbow method and silhouette analysis suggest the same number of clusters? Why might they differ?\n",
    "2. Looking at the visualizations, which k value do you think is most appropriate? Why?\n",
    "3. How does the cluster_std parameter in make_blobs affect the optimal k value?\n",
    "4. What are some limitations of these methods for determining the optimal k?\n",
    "\n",
    "# Exercise 3: Analyzing Cluster Stability\n",
    "\n",
    "In this exercise, you'll implement a stability analysis for k-means clustering using the Adjusted Rand Index (ARI) to compare adjacent values of k. This will help you identify regions where the clustering solution is stable.\n",
    "\n",
    "## Setup\n",
    "First, let's import our required libraries and create two different datasets - one with clear clusters and one with ambiguous clusters:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34bc1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate two datasets\n",
    "def create_datasets():\n",
    "    # Clear clusters\n",
    "    X_clear, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.5, random_state=42)\n",
    "    \n",
    "    # Ambiguous clusters\n",
    "    X_ambig, _ = make_blobs(n_samples=300, centers=4, cluster_std=2.0, random_state=42)\n",
    "    \n",
    "    return X_clear, X_ambig\n",
    "\n",
    "X_clear, X_ambig = create_datasets()\n",
    "\n",
    "# Visualize the datasets\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax1.scatter(X_clear[:, 0], X_clear[:, 1])\n",
    "ax1.set_title(\"Clear Clusters\")\n",
    "ax2.scatter(X_ambig[:, 0], X_ambig[:, 1])\n",
    "ax2.set_title(\"Ambiguous Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1f46bf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 1: Implement Adjacent Cluster Stability Analysis\n",
    "Complete the function to analyze stability between adjacent k values:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81bba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clustering_stability(X, k_range):\n",
    "    \"\"\"\n",
    "    Analyze clustering stability by comparing adjacent k values.\n",
    "    Returns ARI scores between consecutive values of k.\n",
    "    \"\"\"\n",
    "    stability_scores = []\n",
    "    prev_labels = None\n",
    "    \n",
    "    for k in k_range:\n",
    "        # Create and fit KMeans\n",
    "        # Hint: Use a fixed random_state for reproducibility\n",
    "        kmeans = # YOUR CODE HERE\n",
    "        labels = # YOUR CODE HERE\n",
    "        \n",
    "        if prev_labels is not None:\n",
    "            # Calculate ARI between current and previous clustering\n",
    "            # Hint: Use adjusted_rand_score\n",
    "            ari = # YOUR CODE HERE\n",
    "            stability_scores.append(ari)\n",
    "            \n",
    "        prev_labels = labels\n",
    "    \n",
    "    return stability_scores\n",
    "\n",
    "# Test the function on both datasets\n",
    "k_range = range(2, 8)\n",
    "stability_clear = analyze_clustering_stability(X_clear, k_range)\n",
    "stability_ambig = analyze_clustering_stability(X_ambig, k_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e1bdf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 2: Visualize Stability Results\n",
    "Complete the function to create a comparative visualization:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b1b75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stability_comparison(k_range, stability_clear, stability_ambig):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create x-axis labels for plotting\n",
    "    # We have one fewer stability score than k values\n",
    "    x_labels = [f\"{k-1}-{k}\" for k in list(k_range)[1:]]\n",
    "    \n",
    "    # Plot stability scores\n",
    "    # Hint: Use plt.plot with appropriate labels and markers\n",
    "    plt.plot(# YOUR CODE HERE)  # Clear clusters\n",
    "    plt.plot(# YOUR CODE HERE)  # Ambiguous clusters\n",
    "    \n",
    "    plt.xlabel('Comparison between k values')\n",
    "    plt.ylabel('ARI Score')\n",
    "    plt.title('Clustering Stability Analysis')\n",
    "    plt.xticks(range(len(x_labels)), x_labels)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_stability_comparison(k_range, stability_clear, stability_ambig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67283bf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 3: Identify Stability Plateau\n",
    "Implement a function to automatically identify the stability plateau:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80535e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_stability_plateau(stability_scores, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Find the point where stability scores reach a plateau.\n",
    "    Returns the k value at the start of the plateau.\n",
    "    \"\"\"\n",
    "    # Calculate differences between consecutive stability scores\n",
    "    score_differences = np.diff(stability_scores)\n",
    "    \n",
    "    # Find where differences become small\n",
    "    # Hint: Look for where consecutive differences stay below a small value\n",
    "    plateau_start = # YOUR CODE HERE\n",
    "    \n",
    "    # The plateau_start index corresponds to the k value minus 2\n",
    "    # (because we started at k=2 and have one fewer stability score)\n",
    "    k_plateau = plateau_start + 3\n",
    "    \n",
    "    return k_plateau\n",
    "\n",
    "# Find plateaus for both datasets\n",
    "k_plateau_clear = find_stability_plateau(stability_clear)\n",
    "k_plateau_ambig = find_stability_plateau(stability_ambig)\n",
    "\n",
    "print(f\"Stability plateau for clear clusters starts at k={k_plateau_clear}\")\n",
    "print(f\"Stability plateau for ambiguous clusters starts at k={k_plateau_ambig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6b8d11",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 4: Visualize Clustering Results\n",
    "Create a function to visualize the clustering results at different k values:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c883349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clustering_progression(X, k_range):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, k in enumerate(k_range[:6]):  # Plot first 6 k values\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        # Create scatter plot\n",
    "        # Hint: Use different colors for different clusters\n",
    "        axes[idx].scatter(# YOUR CODE HERE)\n",
    "        axes[idx].set_title(f'k={k}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize clustering progression for both datasets\n",
    "plot_clustering_progression(X_clear, k_range)\n",
    "plot_clustering_progression(X_ambig, k_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9791cd1a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 5: Create Stability Summary\n",
    "Implement a function to provide a comprehensive stability analysis:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21969aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_stability_summary(X, k_range):\n",
    "    # Calculate stability scores\n",
    "    stability_scores = analyze_clustering_stability(X, k_range)\n",
    "    \n",
    "    # Find plateau\n",
    "    k_plateau = find_stability_plateau(stability_scores)\n",
    "    \n",
    "    # Calculate additional metrics at plateau k\n",
    "    kmeans = KMeans(n_clusters=k_plateau, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    \n",
    "    # Create summary dictionary\n",
    "    summary = {\n",
    "        'stability_scores': stability_scores,\n",
    "        'k_plateau': k_plateau,\n",
    "        'inertia': kmeans.inertia_,\n",
    "        'cluster_sizes': # YOUR CODE HERE  # Count points in each cluster\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Create summaries for both datasets\n",
    "summary_clear = analyze_stability_summary(X_clear, k_range)\n",
    "summary_ambig = analyze_stability_summary(X_ambig, k_range)\n",
    "\n",
    "# Print summaries\n",
    "for name, summary in [('Clear clusters', summary_clear), \n",
    "                     ('Ambiguous clusters', summary_ambig)]:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"Stability plateau at k={summary['k_plateau']}\")\n",
    "    print(f\"Cluster sizes: {summary['cluster_sizes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42e9662",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Discussion Questions\n",
    "1. How does the stability progression differ between the clear and ambiguous datasets?\n",
    "2. Why might we prefer stability analysis over the elbow method or silhouette analysis?\n",
    "3. What are the limitations of using stability between adjacent k values as a metric?\n",
    "4. How could we make the plateau detection more robust?\n",
    "\n",
    "\n",
    "# Exercise 4: Customer Segmentation with Real Data\n",
    "\n",
    "In this exercise, you'll work with a real customer dataset to perform market segmentation. You'll need to handle data preprocessing, feature selection, and scaling before applying k-means clustering.\n",
    "\n",
    "## Setup\n",
    "First, let's load and examine our customer data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5a8b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the customer data\n",
    "data = pd.DataFrame({\n",
    "    'age': np.random.normal(45, 15, 1000),  # Age\n",
    "    'income': np.random.normal(70000, 30000, 1000),  # Annual income\n",
    "    'spending_score': np.random.normal(50, 25, 1000),  # Spending score (1-100)\n",
    "    'purchase_frequency': np.random.normal(10, 5, 1000),  # Purchases per month\n",
    "    'last_purchase': np.random.normal(30, 20, 1000),  # Days since last purchase\n",
    "})\n",
    "\n",
    "# Add some correlations to make it more realistic\n",
    "data['spending_score'] += data['income'] * 0.0001\n",
    "data['purchase_frequency'] += data['spending_score'] * 0.1\n",
    "\n",
    "print(\"Sample of the customer data:\")\n",
    "print(data.head())\n",
    "print(\"\\nData summary:\")\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a3876",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 1: Data Preprocessing\n",
    "Complete the function to preprocess the data for clustering:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d28ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    # Check for missing values\n",
    "    print(\"Missing values:\\n\", data.isnull().sum())\n",
    "    \n",
    "    # Remove outliers using IQR method\n",
    "    # Hint: Use quantile to find Q1 and Q3\n",
    "    Q1 = # YOUR CODE HERE\n",
    "    Q3 = # YOUR CODE HERE\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Create mask for rows to keep\n",
    "    mask = # YOUR CODE HERE  # Filter out rows outside 1.5*IQR\n",
    "    \n",
    "    data_cleaned = data[mask].copy()\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(data_cleaned)\n",
    "    \n",
    "    return scaled_features, data_cleaned, scaler\n",
    "\n",
    "# Preprocess the data\n",
    "scaled_features, data_cleaned, scaler = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7676cb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 2: Feature Analysis\n",
    "Implement a function to analyze feature relationships:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb32c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_features(scaled_features, original_data):\n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = original_data.corr()\n",
    "    \n",
    "    # Create correlation heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(corr_matrix, cmap='coolwarm', aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns, rotation=45)\n",
    "    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Perform PCA to analyze feature importance\n",
    "    pca = PCA()\n",
    "    pca_result = pca.fit_transform(scaled_features)\n",
    "    \n",
    "    # Calculate explained variance ratio\n",
    "    explained_variance = # YOUR CODE HERE\n",
    "    \n",
    "    # Plot explained variance ratio\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(range(len(explained_variance)), explained_variance)\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('PCA Explained Variance Ratio')\n",
    "    plt.show()\n",
    "    \n",
    "    return pca, explained_variance\n",
    "\n",
    "# Analyze features\n",
    "pca, explained_variance = analyze_features(scaled_features, data_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec149a0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 3: Customer Segmentation\n",
    "Implement the clustering with optimal k selection:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda8c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(scaled_features, k_range=range(2, 11)):\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    \n",
    "    silhouette_scores = []\n",
    "    inertias = []\n",
    "    \n",
    "    for k in k_range:\n",
    "        # Create and fit KMeans model\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(scaled_features)\n",
    "        \n",
    "        # Calculate silhouette score and inertia\n",
    "        silhouette_scores.append(# YOUR CODE HERE)\n",
    "        inertias.append(# YOUR CODE HERE)\n",
    "    \n",
    "    # Plot evaluation metrics\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(k_range, silhouette_scores, 'bo-')\n",
    "    ax1.set_xlabel('k')\n",
    "    ax1.set_ylabel('Silhouette Score')\n",
    "    ax1.set_title('Silhouette Analysis')\n",
    "    \n",
    "    ax2.plot(k_range, inertias, 'ro-')\n",
    "    ax2.set_xlabel('k')\n",
    "    ax2.set_ylabel('Inertia')\n",
    "    ax2.set_title('Elbow Method')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Determine optimal k\n",
    "    # Hint: Use both silhouette score and elbow method\n",
    "    optimal_k = # YOUR CODE HERE\n",
    "    \n",
    "    return optimal_k\n",
    "\n",
    "# Find optimal number of clusters\n",
    "optimal_k = perform_clustering(scaled_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad705e3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 4: Analyze and Visualize Segments\n",
    "Complete the function to analyze and visualize the customer segments:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eee8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_segments(scaled_features, data_cleaned, optimal_k, pca):\n",
    "    # Perform final clustering\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "    labels = kmeans.fit_predict(scaled_features)\n",
    "    \n",
    "    # Add cluster labels to the original data\n",
    "    data_with_clusters = data_cleaned.copy()\n",
    "    data_with_clusters['Cluster'] = labels\n",
    "    \n",
    "    # Calculate segment characteristics\n",
    "    segment_analysis = # YOUR CODE HERE  # Group by cluster and calculate means\n",
    "    \n",
    "    print(\"\\nSegment Characteristics:\")\n",
    "    print(segment_analysis)\n",
    "    \n",
    "    # Create scatter plot using first two PCA components\n",
    "    pca_result = pca.transform(scaled_features)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], \n",
    "                         c=labels, cmap='viridis')\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.title('Customer Segments')\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.show()\n",
    "    \n",
    "    return data_with_clusters, segment_analysis\n",
    "\n",
    "# Analyze and visualize segments\n",
    "data_with_clusters, segment_analysis = analyze_segments(scaled_features, \n",
    "                                                      data_cleaned, \n",
    "                                                      optimal_k, \n",
    "                                                      pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e7563",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 5: Create Customer Segment Profiles\n",
    "Create meaningful descriptions for each customer segment:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60dbb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segment_profiles(segment_analysis):\n",
    "    profiles = {}\n",
    "    \n",
    "    for cluster in segment_analysis.index:\n",
    "        # Analyze the characteristics of each cluster\n",
    "        characteristics = segment_analysis.loc[cluster]\n",
    "        \n",
    "        # Create a descriptive profile based on the characteristics\n",
    "        # Hint: Compare values to overall means\n",
    "        profile = # YOUR CODE HERE  # Create meaningful description\n",
    "        \n",
    "        profiles[f\"Segment {cluster}\"] = profile\n",
    "    \n",
    "    return profiles\n",
    "\n",
    "# Create and print segment profiles\n",
    "profiles = create_segment_profiles(segment_analysis)\n",
    "for segment, description in profiles.items():\n",
    "    print(f\"\\n{segment}:\")\n",
    "    print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fffdbd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Discussion Questions\n",
    "1. How did you determine which features were most important for clustering?\n",
    "2. What insights can you draw about the customer segments you identified?\n",
    "3. How might these segments be used to inform marketing strategies?\n",
    "4. What limitations might this segmentation analysis have?\n",
    "\n",
    "\n",
    "# Exercise 5: Visualizing High-Dimensional Clusters\n",
    "\n",
    "In this exercise, you'll work with high-dimensional data and learn to visualize clustering results using different dimensionality reduction techniques. You'll compare PCA, t-SNE, and UMAP for cluster visualization.\n",
    "\n",
    "## Setup\n",
    "First, let's create some high-dimensional data with known cluster structure:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a561bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import seaborn as sns\n",
    "\n",
    "# Create high-dimensional data (20 dimensions) with 5 clusters\n",
    "n_samples = 500\n",
    "n_features = 20\n",
    "n_clusters = 5\n",
    "\n",
    "# Generate data with known cluster structure\n",
    "X, true_labels = make_blobs(n_samples=n_samples, \n",
    "                           n_features=n_features,\n",
    "                           centers=n_clusters,\n",
    "                           cluster_std=1.8,\n",
    "                           random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform k-means clustering\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "print(\"Data shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c261bc37",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 1: Implement PCA Visualization\n",
    "Complete the function to visualize clusters using PCA:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d7d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pca(X_scaled, labels, title):\n",
    "    # Create and fit PCA\n",
    "    # Hint: Use n_components=2 for 2D visualization\n",
    "    pca = # YOUR CODE HERE\n",
    "    X_pca = # YOUR CODE HERE\n",
    "    \n",
    "    # Calculate explained variance ratio\n",
    "    explained_variance = # YOUR CODE HERE\n",
    "    \n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')\n",
    "    plt.colorbar(scatter)\n",
    "    \n",
    "    # Add variance explanation text\n",
    "    variance_text = f'Explained variance: {sum(explained_variance):.2%}'\n",
    "    plt.text(0.02, 0.98, variance_text, transform=plt.gca().transAxes)\n",
    "    \n",
    "    plt.title(f'PCA Visualization: {title}')\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.show()\n",
    "    \n",
    "    return X_pca\n",
    "\n",
    "# Visualize both true labels and cluster assignments\n",
    "X_pca_true = visualize_pca(X_scaled, true_labels, \"True Labels\")\n",
    "X_pca_cluster = visualize_pca(X_scaled, cluster_labels, \"Cluster Assignments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a4c8d2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 2: Implement t-SNE Visualization\n",
    "Complete the function to visualize clusters using t-SNE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a370558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tsne(X_scaled, labels, title, perplexity=30):\n",
    "    # Create and fit t-SNE\n",
    "    # Hint: Use n_components=2 and the given perplexity\n",
    "    tsne = # YOUR CODE HERE\n",
    "    X_tsne = # YOUR CODE HERE\n",
    "    \n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='viridis')\n",
    "    plt.colorbar(scatter)\n",
    "    \n",
    "    # Add perplexity information\n",
    "    plt.text(0.02, 0.98, f'Perplexity: {perplexity}', transform=plt.gca().transAxes)\n",
    "    \n",
    "    plt.title(f't-SNE Visualization: {title}')\n",
    "    plt.xlabel('t-SNE 1')\n",
    "    plt.ylabel('t-SNE 2')\n",
    "    plt.show()\n",
    "    \n",
    "    return X_tsne\n",
    "\n",
    "# Try different perplexity values\n",
    "perplexities = [5, 30, 50]\n",
    "for perp in perplexities:\n",
    "    X_tsne = visualize_tsne(X_scaled, cluster_labels, f\"Perplexity {perp}\", perp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b644fda",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 3: Implement UMAP Visualization\n",
    "Complete the function to visualize clusters using UMAP:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c1cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_umap(X_scaled, labels, title, n_neighbors=15, min_dist=0.1):\n",
    "    # Create and fit UMAP\n",
    "    # Hint: Use n_components=2 and the given n_neighbors and min_dist\n",
    "    reducer = # YOUR CODE HERE\n",
    "    X_umap = # YOUR CODE HERE\n",
    "    \n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=labels, cmap='viridis')\n",
    "    plt.colorbar(scatter)\n",
    "    \n",
    "    # Add parameter information\n",
    "    plt.text(0.02, 0.98, f'n_neighbors: {n_neighbors}, min_dist: {min_dist}', \n",
    "             transform=plt.gca().transAxes)\n",
    "    \n",
    "    plt.title(f'UMAP Visualization: {title}')\n",
    "    plt.xlabel('UMAP 1')\n",
    "    plt.ylabel('UMAP 2')\n",
    "    plt.show()\n",
    "    \n",
    "    return X_umap\n",
    "\n",
    "# Try different parameter combinations\n",
    "params = [(5, 0.1), (15, 0.1), (30, 0.5)]\n",
    "for n_neighbors, min_dist in params:\n",
    "    X_umap = visualize_umap(X_scaled, cluster_labels, \n",
    "                           f\"n={n_neighbors}, d={min_dist}\",\n",
    "                           n_neighbors, min_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bb38a8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 4: Compare Methods Quantitatively\n",
    "Implement a function to evaluate how well each visualization preserves cluster structure:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3214688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_visualization(X_original, X_reduced, labels):\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    from scipy.stats import spearmanr\n",
    "    \n",
    "    # Calculate silhouette score for both original and reduced data\n",
    "    silhouette_original = # YOUR CODE HERE\n",
    "    silhouette_reduced = # YOUR CODE HERE\n",
    "    \n",
    "    # Calculate pairwise distances in both spaces\n",
    "    from sklearn.metrics.pairwise import euclidean_distances\n",
    "    dist_original = euclidean_distances(X_original)\n",
    "    dist_reduced = euclidean_distances(X_reduced)\n",
    "    \n",
    "    # Flatten the distance matrices and calculate correlation\n",
    "    # Hint: Use spearmanr on the flattened upper triangles\n",
    "    correlation = # YOUR CODE HERE\n",
    "    \n",
    "    return silhouette_original, silhouette_reduced, correlation\n",
    "\n",
    "# Compare all three methods\n",
    "results = {\n",
    "    'PCA': evaluate_visualization(X_scaled, X_pca_cluster, cluster_labels),\n",
    "    't-SNE': evaluate_visualization(X_scaled, X_tsne, cluster_labels),\n",
    "    'UMAP': evaluate_visualization(X_scaled, X_umap, cluster_labels)\n",
    "}\n",
    "\n",
    "# Print results\n",
    "for method, (sil_orig, sil_red, corr) in results.items():\n",
    "    print(f\"\\n{method} Results:\")\n",
    "    print(f\"Original Silhouette Score: {sil_orig:.3f}\")\n",
    "    print(f\"Reduced Silhouette Score: {sil_red:.3f}\")\n",
    "    print(f\"Distance Correlation: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba33e3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Task 5: Create a Dashboard View\n",
    "Implement a function to show all visualizations side by side:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df969e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualization_dashboard(X_scaled, labels):\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Generate all three visualizations\n",
    "    # Hint: Modify previous functions to accept an axis parameter\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = # YOUR CODE HERE\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_tsne = # YOUR CODE HERE\n",
    "    \n",
    "    reducer = umap.UMAP(random_state=42)\n",
    "    X_umap = # YOUR CODE HERE\n",
    "    \n",
    "    # Create scatter plots\n",
    "    for ax, (data, title) in zip(axes, [(X_pca, 'PCA'), \n",
    "                                       (X_tsne, 't-SNE'),\n",
    "                                       (X_umap, 'UMAP')]):\n",
    "        scatter = ax.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(f'{title} 1')\n",
    "        ax.set_ylabel(f'{title} 2')\n",
    "    \n",
    "    plt.colorbar(scatter, ax=axes)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create dashboard for both true labels and cluster assignments\n",
    "create_visualization_dashboard(X_scaled, true_labels)\n",
    "create_visualization_dashboard(X_scaled, cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20142f6f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Discussion Questions\n",
    "1. How do the three visualization methods differ in their representation of the clusters?\n",
    "2. Which method best preserves the original cluster structure? Why?\n",
    "3. How do the parameter choices (perplexity, n_neighbors) affect the visualizations?\n",
    "4. What are the trade-offs between these different visualization techniques?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
