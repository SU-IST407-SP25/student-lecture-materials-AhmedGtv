{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means algorithm\n",
    "\n",
    "The following is a simplified version of the k-means algorithm.\n",
    "\n",
    "```\n",
    "1. Initialize k centroids randomly: C = {c_1, c_2, ..., c_k}\n",
    "2. Repeat until convergence:\n",
    "    a. Assign each data point x_i to the closest centroid:\n",
    "       for i = 1 to m:\n",
    "           d = distance(x_i, c_j) for all j\n",
    "           assign x_i to cluster with closest c_j\n",
    "\n",
    "    b. Compute new centroids for the clusters:\n",
    "       for j = 1 to k:\n",
    "           c_j = mean of all points assigned to cluster j\n",
    "\n",
    "3. Return centroids C and cluster assignments for each data point\n",
    "```\n",
    "\n",
    "Where:\n",
    "- $ k $ is the number of clusters.\n",
    "- $ m $ is the number of data points.\n",
    "- $ x_i $ represents each data point.\n",
    "- $ c_j $ represents each centroid.\n",
    "- The `distance` function could be the Euclidean distance or any other distance metric suitable for the dataset.\n",
    "\n",
    "One way to determine convergence is when the assignments of data points to clusters no longer change.  In practice, several strategies might be used:\n",
    "\n",
    "1. **Change in Centroids**: Stop the algorithm when the change in centroid positions is below a small threshold. This means the centroids are barely moving.\n",
    "2. **Maximum Iterations**: Sometimes, to prevent endless loops due to anomalies or certain data structures, a maximum number of iterations is set. Even if the algorithm hasn't fully converged by this point, it will stop.\n",
    "3. **Change in Cost Function**: $ k $-means tries to minimize the within-cluster sum of squares. If the decrease in this cost function between iterations is smaller than a predefined threshold, the algorithm can be stopped.\n",
    "4. **No Change in Assignments**: As mentioned, if no data point changes its cluster assignment between two consecutive iterations, the algorithm has converged.\n",
    "\n",
    "In many implementations, a combination of the above criteria might be used to ensure both efficiency and convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Importance of Initial Centers\n",
    "\n",
    "Choosing initial centroids can have a large impact on the performance of k-means clustering.  Consider the following examples:\n",
    "\n",
    "### A good clustering result\n",
    "\n",
    "| ![image1](assets/Slide36_Image1.png) | ![image2](assets/Slide36_Image2.png) | ![image3](assets/Slide36_Image3.png) |\n",
    "|---|---|---|\n",
    "| ![image4](assets/Slide36_Image4.png) | ![image5](assets/Slide36_Image5.png) | ![image6](assets/Slide36_Image6.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A not so good clustering result\n",
    "\n",
    "| ![image1](assets/Slide38_Image1.png) | ![image2](assets/Slide38_Image2.png) | ![image3](assets/Slide38_Image3.png) |\n",
    "|---|---|---|\n",
    "| ![image4](assets/Slide38_Image4.png) | ![image5](assets/Slide38_Image5.png) | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Solutions to the Initial Centroids Problem\n",
    "\n",
    "As illustrated above, selecting initial centroids in $ k $-means clustering is crucial because a poor choice can lead to suboptimal clusterings. Here are some common strategies to address the challenge of choosing initial centroids:\n",
    "\n",
    "1. **Random Initialization**: \n",
    "   - Randomly choose $ k $ data points from the dataset as the initial centroids. \n",
    "   - While simple, this method can sometimes result in poor convergence or convergence to a local minimum.\n",
    "2. **Forgy Method**:\n",
    "   - Similar to random initialization, but instead of picking data points as centroids, $ k $ distinct instances are chosen at random from the dataset.\n",
    "3. **K-Means++**:\n",
    "   - This method aims to spread out the initial centroids.\n",
    "   - First, one centroid is chosen uniformly at random from the data points.\n",
    "   - The next centroid is selected from the remaining data points with probability proportional to the square of the distance from the point to the nearest existing centroid.\n",
    "   - Repeat until $ k $ centroids are chosen.\n",
    "   - K-Means++ tends to lead to better and more consistent results compared to random initialization.\n",
    "4. **Spread Out Initialization**:\n",
    "   - Divide the data space into a grid of small, equally-sized cells.\n",
    "   - Select $ k $ cells randomly and use their center points as initial centroids.\n",
    "5. **Use Hierarchical Clustering**:\n",
    "   - Perform hierarchical clustering on the data.\n",
    "   - Cut the dendrogram to obtain $ k $ clusters.\n",
    "   - Use the centroids of these clusters as the initial centroids for $ k $-means.\n",
    "6. **Sampling and Refinement**:\n",
    "   - Take multiple random samples of the dataset.\n",
    "   - Run $ k $-means on each sample to get centroids.\n",
    "   - Refine these centroids by running them on the full dataset.\n",
    "7. **Binary Splitting**:\n",
    "   - Start with $ k=2 $ and run $ k $-means.\n",
    "   - Choose one of the clusters and split it into two.\n",
    "   - Repeat until you have $ k $ clusters.\n",
    "8. **Use Principal Component Analysis (PCA)**:\n",
    "   - Perform PCA on the dataset to reduce its dimensionality.\n",
    "   - Choose initial centroids from the transformed dataset.\n",
    "\n",
    "The choice of method might depend on the specific nature of the dataset and the problem. K-Means++ is a widely recommended (and the default in sklearn) method because of its general applicability and improved convergence properties compared to standard random initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also requires Scikit-Learn ≥ 1.0.1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means in Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit and predict**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a K-Means clusterer on a dataset if blobs. It will try to find each blob's center and assign each instance to the closest blob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# extra code – the exact arguments of make_blobs() are not important\n",
    "blob_centers = np.array([[ 0.2,  2.3], [-1.5 ,  2.3], [-2.8,  1.8],\n",
    "                         [-2.8,  2.8], [-2.8,  1.3]])\n",
    "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n",
    "X, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std,\n",
    "                  random_state=7)\n",
    "\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves Figure 9–2\n",
    "\n",
    "def plot_clusters(X, y=None):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\", rotation=0)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_clusters(X)\n",
    "plt.gca().set_axisbelow(True)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each instance was assigned to one of the 5 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred is kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the following 5 _centroids_ (i.e., cluster centers) were estimated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `KMeans` instance preserves the labels of the instances it was trained on. Somewhat confusingly, in this context, the _label_ of an instance is the index of the cluster that instance gets assigned to (they are not targets, they are predictions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can predict the labels of new instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
    "kmeans.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Boundaries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model's decision boundaries. This gives us a _Voronoi diagram_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves Figure 9–3\n",
    "\n",
    "def plot_data(X):\n",
    "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
    "\n",
    "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
    "    if weights is not None:\n",
    "        centroids = centroids[weights > weights.max() / 10]\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='o', s=35, linewidths=8,\n",
    "                color=circle_color, zorder=10, alpha=0.9)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=2, linewidths=12,\n",
    "                color=cross_color, zorder=11, alpha=1)\n",
    "\n",
    "def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n",
    "                             show_xlabels=True, show_ylabels=True):\n",
    "    mins = X.min(axis=0) - 0.1\n",
    "    maxs = X.max(axis=0) + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
    "                         np.linspace(mins[1], maxs[1], resolution))\n",
    "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                cmap=\"Pastel2\")\n",
    "    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                linewidths=1, colors='k')\n",
    "    plot_data(X)\n",
    "    if show_centroids:\n",
    "        plot_centroids(clusterer.cluster_centers_)\n",
    "\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\")\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", rotation=0)\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundaries(kmeans, X)\n",
    "save_fig(\"voronoi_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Some of the instances near the edges were probably assigned to the wrong cluster, but overall it looks pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hard Clustering _vs_ Soft Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than arbitrarily choosing the closest cluster for each instance, which is called _hard clustering_, it might be better to measure the distance of each instance to all 5 centroids. This is what the `transform()` method does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.transform(X_new).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that this is indeed the Euclidian distance between each instance and each centroid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code\n",
    "np.linalg.norm(np.tile(X_new, (1, k)).reshape(-1, k, 2)\n",
    "               - kmeans.cluster_centers_, axis=2).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Try clustering the iris data.  Try different numbers of clusters. Print the confusion matrix that compares the cluster labels with the actual labels.  What seems to give the best clustering according to ground truth?  How do you know?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "k = 4\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
