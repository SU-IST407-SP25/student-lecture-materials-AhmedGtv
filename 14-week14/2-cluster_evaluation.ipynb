{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Clusterings\n",
    "\n",
    "Evaluating clusterings is a challenging and highly subjective tasks.  In most cases, there is no \"true\" clustering, and so cluster evaluation is necessarily and *exploratory* process. \n",
    "\n",
    "Perhaps the most important thing to consider in evaluating clustering is the role of domain knowledge when interpreting clusters. The 'best' number of clusters according to data-driven metrics may not be meaningful for the problem you're trying to solve; oftentimes, varying the number of clustering is a diagnostic tool that will help you get a little closer to understanding your data.\n",
    "\n",
    "The next most important thing to remember is the value of visualization - especially when clustering, visual inspection can help you understand not only the performance of clustering, but what sorts of clustering techniques are going to be useful.\n",
    "\n",
    "In the following, we introduce a variety of different techniques for interpreting and examining clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### K-Means and the Elbow Method\n",
    "\n",
    "To select the best model in K-means (the right number of $ k $), we will need a way to evaluate a K-Mean model's performance. Unfortunately, clustering is an unsupervised task, so we do not have the targets. Instead, we use a \"cost function,\" which we seek to minimize. In $ k $-means\\, the typical cost function used is the \"within-cluster sum of squares\" (WCSS) (also sometimes referred to as `inertia`). Formally, the WCSS is defined as:\n",
    "\n",
    "$$\\text{WCSS} = \\sum_{i=1}^{k} \\sum_{x \\in C_i} ||x - \\mu_i||^2$$\n",
    "\n",
    "Where:\n",
    "- $ k $ is the number of clusters.\n",
    "- $ C_i $ represents the $ i^{th}$ cluster.\n",
    "- $ x $ is a data point in cluster $ C_i $\n",
    "- $ \\mu_i $ is the centroid of cluster $ C_i $.\n",
    "- $ ||x - \\mu_i||^2 $ is the squared Euclidean distance between the data point $ x $ and the centroid $ \\mu_i $.\n",
    "\n",
    "The goal of $ k $-means is to assign data points to clusters in a way that the total squared distance between the data points and their corresponding cluster centroids is minimized.\n",
    "\n",
    "In simpler terms, the cost function measures the compactness of the clustering. A smaller WCSS indicates that the data points are closer to the centroids of their respective clusters, implying a more compact clustering.\n",
    "\n",
    "In sklearn, the WCSS is referred to as inertia, and accessed via an attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can easily verify, inertia is the sum of the squared distances between each training instance and its closest centroid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code\n",
    "X_dist = kmeans.transform(X)\n",
    "(X_dist[np.arange(len(X_dist)), kmeans.labels_] ** 2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `score()` method returns the negative inertia. Why negative? Well, it is because a predictor's `score()` method must always respect the \"_greater is better_\" rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.score(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the optimal number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the number of clusters was set to a lower or greater value than 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_k3 = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_k8 = KMeans(n_clusters=8, random_state=42)\n",
    "\n",
    "plot_clusterer_comparison(kmeans_k3, kmeans_k8, X, \"$k=3$\", \"$k=8$\")\n",
    "save_fig(\"bad_n_clusters_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch, these two models don't look great. What about their inertias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_k3.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_k8.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, we cannot simply take the value of $k$ that minimizes the inertia, since it keeps getting lower as we increase $k$. Indeed, the more clusters there are, the closer each instance will be to its closest centroid, and therefore the lower the inertia will be. However, we can plot the inertia as a function of $k$ and analyze the resulting curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves Figure 9–8\n",
    "\n",
    "kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X)\n",
    "                for k in range(1, 10)]\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]\n",
    "\n",
    "plt.figure(figsize=(8, 3.5))\n",
    "plt.plot(range(1, 10), inertias, \"bo-\")\n",
    "plt.xlabel(\"$k$\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.annotate(\"\", xy=(4, inertias[3]), xytext=(4.45, 650),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1))\n",
    "plt.text(4.5, 650, \"Elbow\", horizontalalignment=\"center\")\n",
    "plt.axis([1, 8.5, 0, 1300])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is an elbow at $k=4$, which means that less clusters than that would be bad, and more clusters would not help much and might cut clusters in half. So $k=4$ is a pretty good choice. Of course in this example it is not perfect since it means that the two blobs in the lower left will be considered as just a single cluster, but it's a pretty good clustering nonetheless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code\n",
    "plot_decision_boundaries(kmeans_per_k[4 - 1], X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach to picking $ k $ is known by its technical name: the **elbow method**.  However, the elbow method does not always yeild a very distinct elbow, especially when the data is somewhat noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Use the elbow method to obtain the best clustering for the iris data.  Does your answer match what you determined previously?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Generate synthetic data with 3 clear clusters\n",
    "X1, y1 = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Generate synthetic data with no clear clusters (high standard deviation)\n",
    "X2, y2 = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=5)\n",
    "\n",
    "# Function to apply KMeans and plot elbow graph\n",
    "def plot_elbow(ax, X, title):\n",
    "    distortions = []\n",
    "    K = range(1, 10)\n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters=k,n_init='auto')\n",
    "        kmeans.fit(X)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "    \n",
    "    ax.plot(K, distortions, marker='o')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Number of Clusters (K)')\n",
    "    ax.set_ylabel('Distortion')\n",
    "\n",
    "# Function to plot data\n",
    "def plot_data(ax, X, title):\n",
    "    ax.scatter(X[:, 0], X[:, 1], c='blue', marker='o', edgecolor='black', s=50)\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot data and elbow graphs\n",
    "plot_data(axes[0, 0], X1, 'Clear Clusters')\n",
    "plot_data(axes[0, 1], X2, 'Overlapping Clusters')\n",
    "plot_elbow(axes[1, 0], X1, 'Clear Elbow')\n",
    "plot_elbow(axes[1, 1], X2, 'No Clear Elbow')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it is highly subjective, it's generally advised to use several different methods for evaluating clustering.\n",
    "\n",
    "### Silhouette Method\n",
    "\n",
    "\n",
    "$$\n",
    "s(i) = \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}\n",
    "$$\n",
    "\n",
    "- $ a(i) $ = Average distance from the $ i^{th} $ point to the other points in the same cluster.\n",
    "- $ b(i) $ = Lowest average distance from the $ i^{th} $ point to the points in the other clusters, minimized over clusters.\n",
    "\n",
    "\n",
    "1. **Run Clustering**: Perform clustering for your chosen number of clusters $ k $.\n",
    "   \n",
    "2. **Calculate Silhouette Scores**: Compute the silhouette score for each data point using the formula.\n",
    "\n",
    "3. **Plot Silhouette Values**: Create a silhouette plot to visualize how close each point in one cluster is to the points in the neighboring clusters.\n",
    "\n",
    "4. **Interpret Results**: \n",
    "    - A high average silhouette width indicates a good clustering.\n",
    "    - Silhouette scores range from -1 for incorrect clustering to +1 for highly dense clustering. \n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- Computationally expensive for large datasets.\n",
    "- May not be suitable for non-convex clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting silhouette scores\n",
    "\n",
    "We can inspect silhouette scores using a \"silhouette plot\" or, as with the elbow method, plotting a mean silhouette score for different values of k. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import numpy as np\n",
    "\n",
    "# Function to apply KMeans and plot silhouette graph\n",
    "def plot_silhouette(ax, X, title):\n",
    "    # List to store average silhouette scores for different number of clusters\n",
    "    avg_silhouette_scores = []\n",
    "    K = [3]  # Start from 2 as silhouette score requires at least 2 clusters\n",
    "    \n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters=k,n_init='auto')\n",
    "        cluster_labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        # Calculate the silhouette score for the current number of clusters\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        avg_silhouette_scores.append(silhouette_avg)\n",
    "        \n",
    "        # Compute the silhouette scores for each sample\n",
    "        sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "        \n",
    "        # Plot silhouette scores for each cluster\n",
    "        y_lower = 10  # For space between silhouette plots of individual clusters\n",
    "        for i in range(k):\n",
    "            ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "            ax.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, alpha=0.7)\n",
    "            y_lower = y_upper + 10\n",
    "\n",
    "    ax.set_title(f\"{title} - Average Silhouette Score: {silhouette_avg:.2f}\")\n",
    "    ax.set_xlabel(\"Silhouette Coefficient Values\")\n",
    "    ax.set_ylabel(\"Cluster Label\")\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot data and silhouette graphs\n",
    "plot_data(axes[0, 0], X1, 'Clear Clusters')\n",
    "plot_data(axes[0, 1], X2, 'Overlapping Clusters')\n",
    "plot_silhouette(axes[1, 0], X1, 'Clear Clusters')\n",
    "plot_silhouette(axes[1, 1], X2, 'Overlapping Clusters')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Generate synthetic data with 3 clear clusters\n",
    "X1, y1 = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Generate synthetic data with no clear clusters (high standard deviation)\n",
    "X2, y2 = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=5)\n",
    "\n",
    "# Function to apply KMeans and plot silhouette scores\n",
    "def plot_silhouette(ax, X, title):\n",
    "    silhouette_scores = []\n",
    "    K = range(2, 10)  # silhouette_score requires at least 2 clusters\n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters=k,n_init='auto')\n",
    "        kmeans.fit(X)\n",
    "        silhouette_scores.append(silhouette_score(X, kmeans.labels_))\n",
    "    \n",
    "    ax.plot(K, silhouette_scores, marker='o')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Number of Clusters (K)')\n",
    "    ax.set_ylabel('Silhouette Score')\n",
    "\n",
    "# Function to plot data\n",
    "def plot_data(ax, X, title):\n",
    "    ax.scatter(X[:, 0], X[:, 1], c='blue', marker='o', edgecolor='black', s=50)\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot data and silhouette graphs\n",
    "plot_data(axes[0, 0], X1, 'Clear Clusters')\n",
    "plot_data(axes[0, 1], X2, 'Overlapping Clusters')\n",
    "plot_silhouette(axes[1, 0], X1, 'Clear Clusters')\n",
    "plot_silhouette(axes[1, 1], X2, 'Overlapping Clusters')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Davies-Bouldin method\n",
    "\n",
    "Certainly, the Davies-Bouldin Index is another metric used to evaluate the quality of a clustering algorithm. Unlike the Silhouette Score, lower values of the Davies-Bouldin Index indicate better clustering.\n",
    "\n",
    "#### Calculation\n",
    "\n",
    "1. **Within-cluster Distance**: For each cluster, calculate the average distance of each point in the cluster to the centroid of that cluster. This is denoted as $ S_i $ for cluster $ i $.\n",
    "\n",
    "    $$\n",
    "    S_i = \\frac{1}{n_i} \\sum_{j=1}^{n_i} d(x_j, c_i)\n",
    "    $$\n",
    "\n",
    "    Here, $ n_i $ is the number of points in cluster $ i $, $ x_j $ is a point in cluster $ i $, and $ c_i $ is the centroid of cluster $ i $.\n",
    "\n",
    "2. **Between-cluster Distance**: For each pair of clusters $ i $ and $ j $, calculate the distance between their centroids $ c_i $ and $ c_j $. This is denoted as $ d(c_i, c_j) $.\n",
    "\n",
    "3. **Davies-Bouldin value for a Pair**: For each pair of clusters $ i $ and $ j $, calculate the Davies-Bouldin value $ R_{ij} $ using the formula:\n",
    "\n",
    "    $$\n",
    "    R_{ij} = \\frac{S_i + S_j}{d(c_i, c_j)}\n",
    "    $$\n",
    "\n",
    "4. **Maximal Davies-Bouldin value for Each Cluster**: For each cluster $ i $, find the cluster $ j $ that maximizes $ R_{ij} $. Let this maximum value for cluster $ i $ be $ R_i $.\n",
    "\n",
    "    $$\n",
    "    R_i = \\max_{j \\neq i} R_{ij}\n",
    "    $$\n",
    "\n",
    "5. **Average over All Clusters**: Finally, the Davies-Bouldin Index $ DB $ is the average of $ R_i $ over all clusters.\n",
    "\n",
    "    $$\n",
    "    DB = \\frac{1}{k} \\sum_{i=1}^{k} R_i\n",
    "    $$\n",
    "\n",
    "Here, $ k $ is the number of clusters.\n",
    "\n",
    "\n",
    "#### Limitations and Benefits\n",
    "\n",
    "- **Benefits**: The Davies-Bouldin Index is simple to understand and computationally efficient.\n",
    "  \n",
    "- **Limitations**: It may sometimes favor convex clusters over other types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "# Generate synthetic data with 3 clear clusters\n",
    "X1, y1 = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n",
    "\n",
    "# Generate synthetic data with no clear clusters (high standard deviation)\n",
    "X2, y2 = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=5)\n",
    "\n",
    "# Function to apply KMeans and plot silhouette scores\n",
    "def plot_davies_bouldin(ax, X, title):\n",
    "    db_scores = []\n",
    "    K = range(2, 10)  # Davies-Bouldin is undefined for n_clusters=1\n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters=k,n_init='auto')\n",
    "        kmeans.fit(X)\n",
    "        labels = kmeans.labels_\n",
    "        db_score = davies_bouldin_score(X, labels)\n",
    "        db_scores.append(db_score)\n",
    "    \n",
    "    ax.plot(K, db_scores, marker='o')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Number of Clusters (K)')\n",
    "    ax.set_ylabel('Davies-Bouldin Index')\n",
    "\n",
    "# Function to plot data\n",
    "def plot_data(ax, X, title):\n",
    "    ax.scatter(X[:, 0], X[:, 1], c='blue', marker='o', edgecolor='black', s=50)\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot data and silhouette graphs\n",
    "plot_data(axes[0, 0], X1, 'Clear Clusters')\n",
    "plot_data(axes[0, 1], X2, 'Overlapping Clusters')\n",
    "plot_davies_bouldin(axes[1, 0], X1, 'Clear Clusters')\n",
    "plot_davies_bouldin(axes[1, 1], X2, 'Overlapping Clusters')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Using the data in the data directory, use the data driven clustering methods to decide on the best clustering number. What do you come up with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Inspection\n",
    "\n",
    "As discussed above, you should **always** strive to visualize your data. However, for data with more than three dimensions, we cannot directly plot the clusters for visual inspection. Dimensionality reduction techniques like PCA and t-SNE can help.\n",
    "\n",
    "1. **PCA (Principal Component Analysis)**: \n",
    "    - Linear technique\n",
    "    - Preserves global data structure\n",
    "    - Fast and works well for high-dimensional data\n",
    "    - May not capture complex relationships between features\n",
    "\n",
    "2. **t-SNE (t-Distributed Stochastic Neighbor Embedding)**:\n",
    "    - Non-linear technique\n",
    "    - Preserves local data structure\n",
    "    - Computationally intensive\n",
    "    - Good for capturing complex relationships but may introduce artifacts\n",
    "\n",
    "#### Best Practices:\n",
    "\n",
    "1. **Standardization**: Always standardize your data before applying dimensionality reduction techniques.\n",
    "\n",
    "2. **Multiple Views**: Use multiple methods for dimensionality reduction to get different views of the data.\n",
    "\n",
    "3. **Color Code**: Use color coding to differentiate between clusters and to compare them against known labels, if available.\n",
    "\n",
    "4. **Pair with Metrics**: Always use visual inspection in conjunction with other evaluation metrics for a more robust assessment.\n",
    "\n",
    "#### Limitations:\n",
    "\n",
    "1. **Misleading Interpretations**: Dimensionality reduction can sometimes simplify the data too much, leading to incorrect interpretations.\n",
    "  \n",
    "2. **Computational Cost**: Some methods, like t-SNE, are computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic data with 3 clusters in 10 dimensions\n",
    "X, y = make_blobs(n_samples=300, centers=3, n_features=10, random_state=42, cluster_std=1.0)\n",
    "pd.DataFrame(X).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the dataset\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=3,n_init='auto')\n",
    "kmeans.fit(X_scaled)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Apply PCA for dimensionality reduction to 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Apply t-SNE for dimensionality reduction to 2D\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot original data in 2D PCA space\n",
    "axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=labels, marker='o', edgecolor='black', s=50)\n",
    "axes[0].set_title('PCA')\n",
    "\n",
    "# Plot original data in 2D t-SNE space\n",
    "axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, marker='o', edgecolor='black', s=50)\n",
    "axes[1].set_title('t-SNE')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Use PCA and t-SNE to visualize the sample_data.  What do you see?  Does it help you understand your clustering performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster exploration and Stability Analysis\n",
    "\n",
    "Another potentially valuable technique for investigating different clusterings with a given method is called *stability analysis*.  Stability analysis assesses how consistently a clustering algorithm groups the same or similar data points into the same clusters across different runs. Stability analysis often involves bootstrapping or perturbing the dataset and then applying the clustering algorithm multiple times, but can be used to compare the cluster assignments across different parameter settings.\n",
    "\n",
    "#### Best Practices:\n",
    "\n",
    "1. **Multiple Runs**: Conduct multiple runs with different initial conditions or subsets of the data.\n",
    "  \n",
    "2. **Consensus Clustering**: Use consensus clustering to combine the results of multiple runs into a single clustering that captures the stable structure in the data.\n",
    "\n",
    "3. **Parameter Sensitivity**: Stability analysis can also be used to assess the sensitivity of the clustering to parameter choices.\n",
    "\n",
    "#### Limitations:\n",
    "\n",
    "1. **Computationally Intensive**: Stability analysis requires multiple runs of the clustering algorithm, which can be computationally expensive.\n",
    "\n",
    "2. **False Positives**: Stable clusters are not necessarily meaningful clusters. They might capture noise in a stable way.\n",
    "\n",
    "#### When to Use:\n",
    "\n",
    "- When you have high-dimensional data or complex data structures.\n",
    "- When you're unsure about the reliability of the clustering.\n",
    "- When you're comparing the performance of different clustering algorithms or parameter settings.\n",
    "- When you're assessing the relative \"clusterability\" of different data points.\n",
    "\n",
    "#### Approach\n",
    "- **General idea**: calculate cluster assignments over multiple instances and then compare adjacent clusterings.  \n",
    "- When instances follow an original parameter range, looking for a “plateau” can help identify a decent parameter range\n",
    "\n",
    "To compare adjacent clusterings, one technique is to use the **Adjusted Rand Index**.\n",
    "\n",
    "#### Adjusted Rand Index (ARI)\n",
    "\n",
    "The Adjusted Rand Index is a general measure of the similarity between two data clusterings, adjusted for chance.  It is not necessary for different clusterings to have the same labels, or even the same number of clusters.\n",
    "\n",
    "- ARI = 1: Perfect match\n",
    "- ARI > .65: Marginally acceptable\n",
    "- ARI = 0: Random clustering\n",
    "- ARI < 0: Poor clustering\n",
    "\n",
    "Note that it is possible to calculate a p\\-value for ARI\\, as follows:\n",
    "  * __Compute the ARI for your actual data__ : This will be the value you're testing for significance\\.\n",
    "  * __Random Permutations__ : Shuffle the labels of one of the two clusterings \\(either true or predicted\\) randomly\\, while keeping the other clustering fixed\\. Compute the ARI for each of these permuted sets\\.\n",
    "  * __Generate a Null Distribution__ : Repeat step 2 a large number of times \\(e\\.g\\.\\, 1000 or 10\\,000 times\\) to generate a distribution of ARI scores that you would expect to see purely by chance\\.\n",
    "  * __Calculate p\\-value__ : The p\\-value is the proportion of permuted ARIs that are greater than or equal to the observed ARI\\. A small p\\-value indicates that the observed ARI is significantly different from what would be expected by random chance\\, thus suggesting that the clustering is meaningful\\.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "import numpy as np\n",
    "\n",
    "def perm_test_ari(true_labels, pred_labels, n_perm=1000):\n",
    "    actual_ari = adjusted_rand_score(true_labels, pred_labels)\n",
    "    perm_ari = []\n",
    "    \n",
    "    for i in range(n_perm):\n",
    "        perm_labels = np.random.permutation(pred_labels)\n",
    "        perm_ari.append(adjusted_rand_score(true_labels, perm_labels))\n",
    "        \n",
    "    p_value = (np.sum(np.array(perm_ari) >= actual_ari) + 1) / (n_perm + 1)\n",
    "    \n",
    "    return actual_ari, p_value\n",
    "\n",
    "# Example usage\n",
    "true_labels = [0, 0, 1, 1, 2, 2]\n",
    "pred_labels = [0, 0, 1, 2, 2, 2]  # some clustering result\n",
    "\n",
    "actual_ari, p_value = perm_test_ari(true_labels, pred_labels)\n",
    "\n",
    "print(f\"Actual ARI: {actual_ari}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability Analysis with ARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def create_data(centers, std_dev):\n",
    "    X, y = make_blobs(n_samples=300, centers=centers, cluster_std=std_dev, random_state=42)\n",
    "    return X\n",
    "\n",
    "# Function to run KMeans with different K and check stability\n",
    "def check_stability_for_diff_K(X, K_values, n_runs=5):\n",
    "    ari_scores = []\n",
    "    \n",
    "    for k in K_values:\n",
    "        temp_ari_scores = []\n",
    "        prev_labels = None\n",
    "        \n",
    "        for i in range(n_runs):\n",
    "            kmeans = KMeans(n_clusters=k, random_state=i, n_init=\"auto\")\n",
    "            kmeans.fit(X)\n",
    "            labels = kmeans.labels_\n",
    "\n",
    "            if prev_labels is not None:\n",
    "                ari = adjusted_rand_score(prev_labels, labels)\n",
    "                temp_ari_scores.append(ari)\n",
    "            \n",
    "            prev_labels = labels\n",
    "        \n",
    "        mean_ari = np.mean(temp_ari_scores)\n",
    "        ari_scores.append(mean_ari)\n",
    "    \n",
    "    return ari_scores\n",
    "\n",
    "# K values to test\n",
    "K_values = range(2, 11)\n",
    "\n",
    "X_stable = create_data(centers=5, std_dev=1.0)\n",
    "X_unstable = create_data(centers=5, std_dev=2.0)\n",
    "\n",
    "# Check stability for different K\n",
    "stable_scores = check_stability_for_diff_K(X_stable, K_values)\n",
    "unstable_scores = check_stability_for_diff_K(X_unstable, K_values)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K_values, stable_scores, marker='o', label='Stable Clustering')\n",
    "plt.plot(K_values, unstable_scores, marker='x', label='Unstable Clustering')\n",
    "plt.title('Stability Analysis for Different K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Mean ARI Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
