{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Classifiers\n",
    "\n",
    "## 1. Classifiers vs. Regressors\n",
    "\n",
    "In machine learning, we often deal with two main types of supervised learning tasks: classification and regression. While they share many similarities, their goals and outputs differ:\n",
    "\n",
    "- **Regressors** predict continuous numerical values. For example, predicting house prices or temperature.\n",
    "- **Classifiers** predict discrete categories or classes. For example, determining whether an email is spam or not, or identifying the species of a flower.\n",
    "\n",
    "Despite these differences, classifiers and regressors are closely related:\n",
    "\n",
    "- **From Regression to Classification**: We can turn a regression problem into a classification problem through discretization. For example, we could categorize house prices into \"low\", \"medium\", and \"high\" price ranges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
    "y = np.array([30, 45, 60, 75, 90, 105, 120, 135, 150, 165])\n",
    "\n",
    "# Fit a Decision Tree Regressor\n",
    "regressor = DecisionTreeRegressor().fit(X, y)\n",
    "\n",
    "# Predict\n",
    "predictions = regressor.predict([[3.5], [7.5]])\n",
    "print(\"Regression predictions:\", predictions)\n",
    "\n",
    "# Convert to classifier (discretization)\n",
    "def classify_price(price):\n",
    "    if price < 80:\n",
    "        return \"low\"\n",
    "    elif price < 140:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "classified_predictions = [classify_price(p) for p in predictions]\n",
    "print(\"Classified predictions:\", classified_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **From Classification to Regression**: Similarly, we can turn a classification problem into a regression problem through interpolation. For instance, a DecisionTreeClassifier can be modified to output probabilities (which are continuous values) instead of just class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
    "y = np.array(['low', 'low', 'low', 'medium', 'medium', 'medium', 'medium', 'high', 'high', 'high'])\n",
    "\n",
    "# Fit a Decision Tree Classifier\n",
    "classifier = DecisionTreeClassifier().fit(X, y)\n",
    "\n",
    "# Predict probabilities (continuous values)\n",
    "probabilities = classifier.predict_proba([[3.5], [7.5]])\n",
    "print(\"Classification probabilities:\", probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Binary vs. Multiclass Classifiers\n",
    "\n",
    "Classifiers can be categorized based on the number of classes they can handle:\n",
    "\n",
    "- **Binary Classifiers**: These deal with two-class problems. Examples include spam detection (spam or not spam) and medical diagnosis (disease present or absent).\n",
    "\n",
    "- **Multiclass Classifiers**: These can handle problems with more than two classes. For instance, classifying handwritten digits (0-9) or species of flowers.\n",
    "\n",
    "Some algorithms naturally support multiclass classification (like Decision Trees), while others are inherently binary (like Logistic Regression) but can be adapted for multiclass problems using techniques like One-vs-Rest or One-vs-One."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "import textwrap\n",
    "\n",
    "# Load iris dataset (multiclass)\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Binary classifier adapted for multiclass\n",
    "multiclass_classifier = OneVsRestClassifier(LogisticRegression()).fit(X, y)\n",
    "\n",
    "# Predict\n",
    "def pprint_preds(p):\n",
    "    print(\"Iris Predictions\")\n",
    "    print(\"----------------\")\n",
    "    print(textwrap.fill(\",\".join(predictions)))\n",
    "\n",
    "pprint_preds(multiclass_classifier.predict(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Instance-based vs. Model-based Learning\n",
    "\n",
    "Classifiers can also be categorized based on how they learn and make predictions:\n",
    "\n",
    "- **Instance-based Learning**: These algorithms don't explicitly learn a model. Instead, they memorize the training instances and use them directly for prediction. Examples include k-Nearest Neighbors (k-NN) and Support Vector Machines (SVMs).\n",
    "\n",
    "- **Model-based Learning**: These algorithms learn an explicit model from the training data, which is then used for predictions. Examples include Decision Trees, Logistic Regression, and Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instance-based (k-NN)\n",
    "knn = KNeighborsClassifier(n_neighbors=3).fit(X, y)\n",
    "print(\"KNN\")\n",
    "pprint_preds(knn.predict(X))\n",
    "\n",
    "# Model-based (Decision Tree)\n",
    "dt = DecisionTreeClassifier().fit(X, y)\n",
    "print(\"Decision Tree\")\n",
    "pprint_preds(dt.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating Classifiers vs. Regressors\n",
    "\n",
    "The evaluation metrics for classifiers and regressors differ:\n",
    "\n",
    "- **Regressor Metrics**: Common metrics include Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R-squared.\n",
    "\n",
    "- **Classifier Metrics**: These include Accuracy, Precision, Recall, F1-score, and Area Under the ROC Curve (AUC-ROC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# For regression\n",
    "X_reg, y_reg = load_wine(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reg, y_reg, test_size=0.2)\n",
    "reg_model = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "reg_predictions = reg_model.predict(X_test)\n",
    "print(\"MSE:\", mean_squared_error(y_test, reg_predictions))\n",
    "\n",
    "# For classification\n",
    "X_clf, y_clf = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_clf, y_clf, test_size=0.2)\n",
    "clf_model = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "clf_predictions = clf_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, clf_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Other Considerations\n",
    "\n",
    "### Class Imbalance\n",
    "\n",
    "Class imbalance occurs when the classes in a dataset are not represented equally. This can significantly impact both the training and evaluation of classifiers:\n",
    "\n",
    "- **Training**: Most classifiers assume balanced classes. With imbalanced data, they might bias towards the majority class.\n",
    "- **Evaluation**: Accuracy can be misleading with imbalanced data. Metrics like precision, recall, and F1-score are often more informative.\n",
    "\n",
    "Techniques to handle class imbalance include:\n",
    "- Oversampling the minority class (e.g., SMOTE)\n",
    "- Undersampling the majority class\n",
    "- Adjusting class weights in the model\n",
    "- Using appropriate evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Generate imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, weights=[0.9, 0.1], random_state=42)\n",
    "\n",
    "# Train without handling imbalance\n",
    "clf = LogisticRegression().fit(X, y)\n",
    "print(\"Without handling imbalance:\\n\", classification_report(y, clf.predict(X)))\n",
    "\n",
    "# Handle imbalance with SMOTE\n",
    "X_resampled, y_resampled = SMOTE().fit_resample(X, y)\n",
    "clf_balanced = LogisticRegression().fit(X_resampled, y_resampled)\n",
    "print(\"After handling imbalance:\\n\", classification_report(y, clf_balanced.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "While some classifiers (like Decision Trees) are not sensitive to the scale of features, others (like SVM, k-NN, and neural networks) perform better when features are on a similar scale. StandardScaler or MinMaxScaler from sklearn can be used for this purpose.\n",
    "\n",
    "### Handling Missing Data\n",
    "\n",
    "Real-world datasets often contain missing values. Strategies to handle this include:\n",
    "- Removing instances with missing data (if data is abundant)\n",
    "- Imputing missing values (mean, median, or more advanced techniques)\n",
    "- Using algorithms that can handle missing data (like some implementations of Random Forests)\n",
    "\n",
    "### Interpretability\n",
    "\n",
    "Some classifiers (like Decision Trees) provide easily interpretable models, while others (like Neural Networks) are often seen as \"black boxes\". The need for model interpretability should be considered when choosing a classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
