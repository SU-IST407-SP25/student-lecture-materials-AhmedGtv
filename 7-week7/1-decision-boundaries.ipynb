{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Decision Boundaries in Machine Learning\n",
    "\n",
    "Decision boundaries are a fundamental concept in classification problems. They represent the regions in the feature space where a classifier changes its prediction from one class to another. Understanding decision boundaries is crucial for interpreting how different classifiers work and when to use them.\n",
    "\n",
    "## Types of Decision Boundaries\n",
    "\n",
    "### 1. Linear Decision Boundaries\n",
    "\n",
    "Linear decision boundaries are straight lines (in 2D), planes (in 3D), or hyperplanes (in higher dimensions) that separate different classes. Classifiers that produce linear decision boundaries include:\n",
    "\n",
    "- Logistic Regression\n",
    "- Linear Support Vector Machines (SVM)\n",
    "- Perceptrons\n",
    "\n",
    "Linear boundaries are simple and often work well for linearly separable data. However, they can struggle with more complex, non-linear relationships in the data.\n",
    "\n",
    "### 2. Non-Linear Decision Boundaries\n",
    "\n",
    "Non-linear decision boundaries can take various shapes and are not restricted to straight lines or planes. They can be:\n",
    "\n",
    "- Continuous curves or surfaces\n",
    "- Discontinuous regions\n",
    "\n",
    "Classifiers that can produce non-linear decision boundaries include:\n",
    "\n",
    "- Non-linear SVMs (with kernels)\n",
    "- Decision Trees and Random Forests\n",
    "- k-Nearest Neighbors (k-NN)\n",
    "- Neural Networks\n",
    "\n",
    "Non-linear boundaries can capture more complex relationships in the data but may be prone to overfitting if not properly regularized.\n",
    "\n",
    "## Visualizing Decision Boundaries\n",
    "\n",
    "Let's use Python to visualize decision boundaries for different classifiers. We'll use the `make_moons` function from sklearn to generate non-linearly separable data and compare how different classifiers perform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "def plot_decision_boundary(ax, clf, X, y, title):\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.8)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolor='black')\n",
    "    ax.set_xlabel(\"Feature 1\")\n",
    "    ax.set_ylabel(\"Feature 2\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Generate datasets\n",
    "X, y = make_moons(noise=0.3, random_state=0)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "# Create and train classifiers\n",
    "classifiers = [\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    KNeighborsClassifier(3)\n",
    "]\n",
    "\n",
    "titles = [\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"Gaussian Process\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "    \"k-NN\"\n",
    "]\n",
    "\n",
    "# Plot decision boundaries\n",
    "fig, axs = plt.subplots(5, 2, figsize=(20, 30))\n",
    "axs = axs.ravel()  # Flatten the 2D array of axes\n",
    "\n",
    "for ax, clf, title in zip(axs, classifiers, titles):\n",
    "    clf.fit(X_train, y_train)\n",
    "    plot_decision_boundary(ax, clf, X, y, title)\n",
    "\n",
    "# Remove any unused subplots\n",
    "for ax in axs[len(classifiers):]:\n",
    "    fig.delaxes(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the Results\n",
    "\n",
    "1. **Linear SVM**: You'll notice that the linear SVM struggles with this non-linear data, creating a straight line that doesn't capture the moon shapes well.\n",
    "\n",
    "2. **RBF SVM**: The RBF (Radial Basis Function) kernel allows the SVM to create a non-linear boundary that fits the data much better.\n",
    "\n",
    "3. **Decision Tree**: The decision tree creates a \"staircase\" boundary, with horizontal and vertical lines. This can approximate non-linear boundaries but may not be smooth.\n",
    "\n",
    "4. **Random Forest**: By combining multiple decision trees, the random forest creates a smoother, more nuanced boundary.\n",
    "\n",
    "5. **Neural Network**: The neural network can create complex, non-linear boundaries that fit the data well.\n",
    "\n",
    "6. **k-NN**: The k-Nearest Neighbors algorithm creates a boundary that's highly dependent on local data points, resulting in a complex, potentially discontinuous boundary.\n",
    "\n",
    "## Limitations and Considerations\n",
    "\n",
    "1. **Overfitting**: Very complex, non-linear boundaries (like those from k-NN or deep neural networks) can overfit the training data, leading to poor generalization.\n",
    "\n",
    "2. **Underfitting**: Simple, linear boundaries may underfit complex data, missing important patterns.\n",
    "\n",
    "3. **Interpretability**: Linear boundaries are often more interpretable than complex, non-linear ones.\n",
    "\n",
    "4. **Computational Complexity**: Creating non-linear boundaries (especially with methods like SVM with kernels or deep neural networks) can be computationally expensive.\n",
    "\n",
    "## Example: Linear Data with Non-Linear Classifier\n",
    "\n",
    "Let's see what happens when we use a non-linear classifier (Decision Tree) on clearly linear data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate data with a diagonal decision boundary\n",
    "def generate_diagonal_data(n_samples=1000, noise=0.1):\n",
    "    X = np.random.rand(n_samples, 2)\n",
    "    y = (X[:, 1] > X[:, 0]).astype(int)\n",
    "    \n",
    "    # Add some noise\n",
    "    flip_indices = np.random.choice(n_samples, size=int(noise * n_samples), replace=False)\n",
    "    y[flip_indices] = 1 - y[flip_indices]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Plot decision boundary\n",
    "def plot_decision_boundary(ax, clf, X, y, title):\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.8)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolor='black')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', lw=2)  # Plot the true decision boundary\n",
    "    ax.set_xlabel(\"Feature 1\")\n",
    "    ax.set_ylabel(\"Feature 2\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_diagonal_data(n_samples=1000, noise=0.05)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train classifiers with different max_depth\n",
    "max_depths = [2, 3, 5, None]\n",
    "classifiers = [DecisionTreeClassifier(max_depth=depth, random_state=42) for depth in max_depths]\n",
    "\n",
    "# Plot decision boundaries\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for ax, clf, depth in zip(axs, classifiers, max_depths):\n",
    "    clf.fit(X_train, y_train)\n",
    "    plot_decision_boundary(ax, clf, X, y, f\"Decision Tree (max_depth={depth})\")\n",
    "    train_score = clf.score(X_train, y_train)\n",
    "    test_score = clf.score(X_test, y_test)\n",
    "    ax.text(0.05, 0.95, f\"Train score: {train_score:.2f}\\nTest score: {test_score:.2f}\", \n",
    "            transform=ax.transAxes, va='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print feature importances\n",
    "for depth, clf in zip(max_depths, classifiers):\n",
    "    print(f\"\\nFeature importances (max_depth={depth}):\")\n",
    "    for i, importance in enumerate(clf.feature_importances_):\n",
    "        print(f\"Feature {i+1}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, you'll see that the Decision Tree creates a \"staircase\" boundary even though the data is linearly separable. This demonstrates how some non-linear classifiers might struggle to generalize simple linear patterns efficiently.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Understanding decision boundaries is crucial for selecting and interpreting machine learning models. While linear boundaries are simple and interpretable, they may not capture complex relationships in the data. Non-linear boundaries can model more complex patterns but risk overfitting and may be less interpretable. The choice of classifier and resulting decision boundary should be based on the nature of your data, the complexity of the problem, and the trade-offs between model performance, interpretability, and computational resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
