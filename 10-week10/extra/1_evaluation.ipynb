{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation in Machine Learning\n",
    "\n",
    "Generally speaking, we don't make final evaluations on our _training set_.  Instead, what we care most about is how well our model generalizes to unseen data. Thus, we usually hold some data in reserve (which we call \"held-out\" or \"test\" data), train on the non-test data (called the \"training data\"), and evaluate on the test data.  The gold standard for this is \"cross-validation,\" which performs this process multiple times and then averages scores across the different held-out samples. However, cross-validation is computationally more intensive, and so sometimes we'll use simpler methods (e.g., a single train test split).\n",
    "\n",
    "In this notebook, we'll explore essential techniques for evaluating and validating machine learning models. First though, let's review a couple of different metrics we can use to evaluate regressions.\n",
    "\n",
    "## Scoring Regression Results\n",
    "\n",
    "When evaluating regression models, three commonly used metrics are R² (R-squared), RMSE (Root Mean Square Error), and MAE (Mean Absolute Error). Let's explore these metrics in detail and see how to implement them in Python.\n",
    "\n",
    "### R² (R-squared) Score\n",
    "\n",
    "#### What is R²?\n",
    "\n",
    "R², also known as the coefficient of determination, is a statistical measure that represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It provides an indication of the goodness of fit of a model.\n",
    "\n",
    "- R² ranges from 0 to 1.\n",
    "- An R² of 0 indicates that the model explains none of the variability of the data.\n",
    "- An R² of 1 indicates that the model explains all the variability of the data.\n",
    "\n",
    "#### Formula\n",
    "\n",
    "$$ R^2 = 1 - \\frac{SSR}{SST} $$\n",
    "Where:\n",
    "\n",
    "SSR is the sum of squared residuals\n",
    "SST is the total sum of squares\n",
    "\n",
    "More specifically:\n",
    "$$ R^2 = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}i)^2}{\\sum{i=1}^n (y_i - \\bar{y})^2} $$\n",
    "Where:\n",
    "\n",
    "$y_i$ are the observed values\n",
    "$\\hat{y}_i$ are the predicted values\n",
    "$\\bar{y}$ is the mean of the observed data\n",
    "\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- An R² of 0.7 means that 70% of the variance in the target variable can be explained by the model.\n",
    "- Higher R² values indicate a better fit, but be cautious of overfitting when R² is very close to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### RMSE (Root Mean Square Error)\n",
    "\n",
    "RMSE (which we've discussed before) is a frequently used measure of the differences between values predicted by a model and the values actually observed. It represents the standard deviation of the residuals (prediction errors).\n",
    "\n",
    "- RMSE is always non-negative, and a value of 0 indicates a perfect fit to the data.\n",
    "- It has the same units as the dependent variable.\n",
    "- Lower values of RMSE indicate better fit.\n",
    "\n",
    "#### Formula\n",
    "\n",
    "The formula for RMSE is:\n",
    "\n",
    "$$ RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2} $$\n",
    "Where:\n",
    "\n",
    "$n$ is the number of observations\n",
    "$y_i$ are the observed values\n",
    "$\\hat{y}_i$ are the predicted values\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- RMSE can be interpreted as the average deviation of the predictions from the observed values.\n",
    "- It gives more weight to large errors due to the squaring operation.\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "Mean Absolute Error (MAE) is another common metric used to evaluate regression models. It measures the average magnitude of the errors in a set of predictions, without considering their direction. \n",
    "\n",
    "#### Formula\n",
    "\n",
    "The formula for MAE is:\n",
    "\n",
    "$$ MAE = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i| $$\n",
    "\n",
    "Where:\n",
    "- $n$ is the number of observations\n",
    "- $y_i$ are the observed values\n",
    "- $\\hat{y}_i$ are the predicted values\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "- MAE is always non-negative, and a value of 0 indicates a perfect fit to the data.\n",
    "- It has the same units as the dependent variable.\n",
    "- Lower values of MAE indicate better fit.\n",
    "- MAE represents the average absolute difference between predicted and actual values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Comparing MAE and RMSE\n",
    "\n",
    "Both MAE and RMSE are commonly used metrics for regression problems, but they have some key differences:\n",
    "\n",
    "1. **Interpretation**: \n",
    "   - MAE is easier to interpret as it's in the same units as the target variable and represents the average absolute error.\n",
    "   - RMSE is in the same units as the target variable, but it represents the standard deviation of the residuals.\n",
    "\n",
    "2. **Sensitivity to outliers**:\n",
    "   - MAE is less sensitive to outliers because it doesn't square the errors.\n",
    "   - RMSE gives higher weight to large errors due to the squaring operation, making it more sensitive to outliers.\n",
    "\n",
    "3. **Mathematical properties**:\n",
    "   - MAE is based on the L1 norm (sum of absolute values).\n",
    "   - RMSE is based on the L2 norm (sum of squared values).\n",
    "\n",
    "4. **Formula comparison**:\n",
    "   MAE: $\\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|$\n",
    "   RMSE: $\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}$\n",
    "\n",
    "5. **Use cases**:\n",
    "   - MAE is preferred when you want to treat all errors equally.\n",
    "   - RMSE is preferred when large errors are particularly undesirable, as it penalizes them more heavily.\n",
    "\n",
    "\n",
    "#### Choosing Between MAE and RMSE\n",
    "\n",
    "- Use MAE when you want to treat all errors equally and when outliers are not particularly problematic for your application.\n",
    "- Use RMSE when large errors are especially undesirable, or when you want to maintain mathematical properties like differentiability (RMSE is differentiable everywhere, while MAE is not differentiable at 0).\n",
    "- Often, it's beneficial to report both metrics to provide a more comprehensive view of your model's performance.\n",
    "\n",
    "Remember, the choice between MAE and RMSE (or using both) can depend on your specific problem, the nature of your data, and the requirements of your stakeholders.\n",
    "\n",
    "\n",
    "## Using sklearn\n",
    "\n",
    "Scikit-learn provides easy to use implementations most common metrics in the \"metrics\" package of the library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.9486\n",
      "RMSE: 0.6124\n",
      "MAE: 0.5000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.array([3, -0.5, 2, 7])\n",
    "y_pred = np.array([2.5, 0.0, 2, 8])\n",
    "\n",
    "# R² score\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "# MAE\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(f\"MAE: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Train/Test Splits\n",
    "\n",
    "Train/test splits are crucial in machine learning for several reasons:\n",
    "1. They help us estimate how well our model will perform on unseen data.\n",
    "2. They prevent overfitting by evaluating the model on data it hasn't seen during training.\n",
    "3. They provide a way to compare different models fairly.\n",
    "\n",
    "### Implementing train/test split in raw Python\n",
    "\n",
    "Let's start by implementing a simple train/test split function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (80, 1), Test set shape: (20, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# This initializes the random state for reproduceability \n",
    "np.random.seed(42)\n",
    "\n",
    "def my_train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    \n",
    "    \n",
    "    n_samples = len(X)\n",
    "    n_test = int(n_samples * test_size)\n",
    "    \n",
    "    # Create random indices\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    test_indices = indices[:n_test]\n",
    "    train_indices = indices[n_test:]\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Example usage\n",
    "X = np.random.rand(100, 1)  # 100 samples, 1 feature\n",
    "y = 2 * X + 1 + np.random.randn(100, 1) * 0.1  # Linear relationship with some noise\n",
    "\n",
    "X_train, X_test, y_train, y_test = my_train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this train test split, we can now train and evaluate a predictor.  We'll use sklearn's linear regression for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.012927141660667569\n",
      "Explained Variance = 0.9452137743424628\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "print(f\"RMSE = {mean_squared_error(y_test,y_pred)}\")\n",
    "print(f\"Explained Variance = {r2_score(y_test, y_pred)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that different splits will return different scores because they are randomly chosen.  This is one reason why we use cross validation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1. RMSE = 0.006067782758113735\n",
      "Trial 1. Explained Variance = 0.9840161691700738\n",
      "\n",
      "Trial 2. RMSE = 0.008795323257008715\n",
      "Trial 2. Explained Variance = 0.9792219687827336\n",
      "\n",
      "Trial 3. RMSE = 0.010294716137719685\n",
      "Trial 3. Explained Variance = 0.9750807417475278\n",
      "\n",
      "Trial 4. RMSE = 0.007274012847573889\n",
      "Trial 4. Explained Variance = 0.9788703444365846\n",
      "\n",
      "Trial 5. RMSE = 0.006743404542508677\n",
      "Trial 5. Explained Variance = 0.972488694087248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for trial in range(5):\n",
    "    X_train, X_test, y_train, y_test = my_train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = lr.predict(X_test)\n",
    "    print(f\"Trial {trial+1}. RMSE = {mean_squared_error(y_test,y_pred)}\")\n",
    "    print(f\"Trial {trial+1}. Explained Variance = {r2_score(y_test, y_pred)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Using scikit-learn for train/test split\n",
    "\n",
    "The same result can be achieved more concisely with scikit learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1. RMSE = 0.006536995137170029\n",
      "Trial 1. Explained Variance = 0.9825431689004598\n",
      "\n",
      "Trial 2. RMSE = 0.014438846524693839\n",
      "Trial 2. Explained Variance = 0.9458126172986798\n",
      "\n",
      "Trial 3. RMSE = 0.003921147362602184\n",
      "Trial 3. Explained Variance = 0.9808558934078107\n",
      "\n",
      "Trial 4. RMSE = 0.00534263214183482\n",
      "Trial 4. Explained Variance = 0.9876195998568261\n",
      "\n",
      "Trial 5. RMSE = 0.00958719625066688\n",
      "Trial 5. Explained Variance = 0.9648974460279144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(42)\n",
    "\n",
    "for trial in range(5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = lr.predict(X_test)\n",
    "    print(f\"Trial {trial+1}. RMSE = {mean_squared_error(y_test,y_pred)}\")\n",
    "    print(f\"Trial {trial+1}. Explained Variance = {r2_score(y_test, y_pred)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As you can see, scikit-learn's implementation is more concise and likely more optimized.\n",
    "\n",
    "## 3. Cross-Validation\n",
    "\n",
    "### Why do we need cross-validation?\n",
    "\n",
    "Cross-validation helps us:\n",
    "1. Get a more robust estimate of model performance by using multiple train/test splits.\n",
    "2. Reduce the impact of data variability on model evaluation.\n",
    "3. Make better use of limited data for both training and validation.\n",
    "\n",
    "### Implementing cross-validation in raw Python\n",
    "\n",
    "Let's implement a simple 5-fold cross-validation function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation RMSE scores: [np.float64(0.012907383102223256), np.float64(0.005676334975620066), np.float64(0.008995239911145073), np.float64(0.013058370647536618), np.float64(0.013084154878304746)]\n",
      "Mean RMSE: 0.0107\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def five_fold_cross_validation(X, y, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    n_samples = len(X)\n",
    "    fold_size = n_samples // 5\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    \n",
    "    for i in range(5):\n",
    "        test_start = i * fold_size\n",
    "        test_end = (i + 1) * fold_size if i < 4 else n_samples\n",
    "        \n",
    "        test_indices = indices[test_start:test_end]\n",
    "        train_indices = np.concatenate([indices[:test_start], indices[test_end:]])\n",
    "        \n",
    "        yield X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "\n",
    "X = np.random.rand(100, 1)  # 100 samples, 1 feature\n",
    "y = 2 * X + 1 + np.random.randn(100, 1) * 0.1 \n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = []\n",
    "for X_train, X_test, y_train, y_test in five_fold_cross_validation(X, y, random_state=42):\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    cv_scores.append(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Cross-validation RMSE scores: {cv_scores}\")\n",
    "print(f\"Mean RMSE: {np.mean(cv_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Using scikit-learn for cross-validation\n",
    "\n",
    "Scikit-learn has lots of tools for handling cross-validation.  `cross_val_score` is an easy one liner that handles most of the simple cases for cross validation.  By default, `cross_val_score` uses R-squared rather than RMSE for regression tasks, but we can pass a string to get one of several predefined scores (see the [docs](https://scikit-learn.org/stable/modules/model_evaluation.html)) or even pass a scoring function.  By default, metrics specified as string in `cross_val_score` are written as fitness functions, so that \"good\" values are higher.  Hence, instead of reporting RMSE, we get _negative_ RMSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation neg RMSE scores: [-0.00762899 -0.01442171 -0.01197623 -0.01278082 -0.00685235]\n",
      "Mean neg RMSE: -0.0107\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "print(f\"Cross-validation neg RMSE scores: {cv_scores}\")\n",
    "print(f\"Mean neg RMSE: {np.mean(cv_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `cross_val_score` only reports one measure (passed as the 'scoring' parameter).  If you want to pass more than one measure, you can use the `cross_validate` method. The `cross_validate` API requires a scorer (rather that a simple metric), so we use the `make_scorer` method to turn a metric into a scorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time: 0.001 (+/- 0.000)\n",
      "score_time: 0.001 (+/- 0.000)\n",
      "test_R2: 0.980 (+/- 0.012)\n",
      "train_R2: 0.983 (+/- 0.003)\n",
      "test_RMSE: 0.103 (+/- 0.029)\n",
      "train_RMSE: 0.100 (+/- 0.007)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "\n",
    "scoring = {\n",
    "    'R2': make_scorer(lambda y, y_pred: r2_score(y, y_pred)),\n",
    "    'RMSE': make_scorer(lambda y, y_pred: np.sqrt(mean_squared_error(y, y_pred)))\n",
    "}\n",
    "\n",
    "# Using multiple metrics\n",
    "scores = cross_validate(model, X, y, cv=5, scoring=scoring,return_train_score=True)\n",
    "for key, values in scores.items():\n",
    "    print(f\"{key}: {values.mean():.3f} (+/- {values.std() * 2:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **KFold sampler**\n",
    "\n",
    "If you want even more control over your training and testing, you can use the `KFold` class in sklearn.  This works a lot like our python implementation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE: 0.010748635085188005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "# Initialize list to store accuracy for each fold\n",
    "rmse_list = []\n",
    "\n",
    "# Loop through each fold\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    # Split the data into current train and test set\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # It's a good idea to use a fresh, untrained model each time you run on new data\n",
    "    # The \"clone\" command does that, but simplifies things by copying other parameters\n",
    "\n",
    "    lr = LinearRegression()\n",
    "\n",
    "    # Fit the model\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = lr.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    rmse_list.append(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Calculate and print the average accuracy\n",
    "mean_rmse = np.mean(rmse_list)\n",
    "print(f'Average RMSE: {mean_rmse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
