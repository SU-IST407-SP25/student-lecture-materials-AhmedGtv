{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement a Simple Linear Regression class that is compatible with scikit-learn's estimator interface. You'll use gradient descent for optimization, as discussed in the lecture notes.\n",
    "\n",
    "#### Task:\n",
    "\n",
    "Create a `SimpleLinearRegression` class with the following methods:\n",
    "1. `__init__(self, learning_rate=0.01, n_iterations=1000, tolerance=1e-6)`: Initialize the model parameters.\n",
    "2. `fit(self, X, y)`: Fit the model to the training data using gradient descent.\n",
    "3. `predict(self, X)`: Make predictions using the trained model.\n",
    "4. `score(self, X, y)`: Calculate the coefficient of determination R^2 of the prediction.\n",
    "\n",
    "#### Requirements:\n",
    "\n",
    "- The class should inherit from `BaseEstimator` and `RegressorMixin` from scikit-learn.\n",
    "- Use gradient descent to optimize the parameters (weight and bias).\n",
    "- Store the weight as `self.coef_` and the bias as `self.intercept_` (note the trailing underscores).\n",
    "- Implement early stopping in the `fit` method using the `tolerance` parameter.\n",
    "- Ensure that the `fit`, `predict`, and `score` methods work with both 1D and 2D numpy arrays for X.\n",
    "\n",
    "#### Hints:\n",
    "\n",
    "- Remember to reshape input arrays if necessary to ensure consistent dimensions.\n",
    "- The gradient descent update rules for simple linear regression are:\n",
    "  - w = w - learning_rate * (1/n) * sum((y_pred - y) * x)\n",
    "  - b = b - learning_rate * (1/n) * sum(y_pred - y)\n",
    "- You can use `np.mean((y_true - y_pred) ** 2)` to calculate MSE for the stopping criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array\n",
    "\n",
    "class SimpleLinearRegression:\n",
    "    #Implement me\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = SimpleLinearRegression(learning_rate=0.01, n_iterations=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print R-squared score\n",
    "print(f\"R-squared score: {model.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual')\n",
    "plt.plot(X_test, y_pred, color='red', label='Predicted')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Simple Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Coefficient: {model.coef_[0]:.4f}\")\n",
    "print(f\"Intercept: {model.intercept_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will extend the Simple Linear Regression implementation from Exercise 4 to create a Multiple Linear Regression class. This class will also be compatible with scikit-learn's estimator interface and use gradient descent for optimization.\n",
    "\n",
    "#### Task:\n",
    "\n",
    "Create a `MultipleLinearRegression` class with the following methods:\n",
    "1. `__init__(self, learning_rate=0.01, n_iterations=1000, tolerance=1e-6)`: Initialize the model parameters.\n",
    "  - the \"tolerance\" parameter is used as a stopping criterion for the gradient descent algorithm. It helps determine when the algorithm should stop iterating, based on how much the model parameters are changing between iterations.\n",
    "  - after each iteration in gradient descent, if the absolute value of the change in coefficients is less than tolerance, we can halt the gradient descent process\n",
    "2. `fit(self, X, y)`: Fit the model to the training data using gradient descent.\n",
    "3. `predict(self, X)`: Make predictions using the trained model.\n",
    "4. `score(self, X, y)`: Calculate the coefficient of determination R^2 of the prediction.\n",
    "\n",
    "#### Requirements:\n",
    "\n",
    "- The class should inherit from `BaseEstimator` and `RegressorMixin` from scikit-learn.\n",
    "- Use gradient descent to optimize the parameters (weights and bias).\n",
    "- Store the weights as `self.coef_` and the bias as `self.intercept_` (note the trailing underscores).\n",
    "- Implement early stopping in the `fit` method using the `tolerance` parameter.\n",
    "- Ensure that the `fit`, `predict`, and `score` methods work with 2D numpy arrays for X.\n",
    "- Handle multiple features in the input data.\n",
    "\n",
    "#### Hints:\n",
    "\n",
    "- The implementation will be very similar to the Simple Linear Regression class, but you'll need to handle multiple features.\n",
    "- The gradient descent update rules for multiple linear regression are:\n",
    "  - w = w - learning_rate * (1/n) * X.T.dot(y_pred - y)\n",
    "  - b = b - learning_rate * (1/n) * sum(y_pred - y)\n",
    "- You can use `np.mean((y_true - y_pred) ** 2)` to calculate MSE for the stopping criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array\n",
    "\n",
    "class MultipleLinearRegression:\n",
    "    # implement me\n",
    "    pass\n",
    "\n",
    "# Test the implementation\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=3, noise=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = MultipleLinearRegression(learning_rate=0.01, n_iterations=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and print R-squared score\n",
    "print(f\"R-squared score: {model.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Plot results (for the first feature)\n",
    "plt.scatter(X_test[:, 0], y_test, color='blue', label='Actual')\n",
    "plt.scatter(X_test[:, 0], y_pred, color='red', label='Predicted')\n",
    "plt.xlabel('X (first feature)')\n",
    "plt.ylabel('y')\n",
    "plt.title('Multiple Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(f\"Intercept: {model.intercept_:.4f}\")\n",
    "\n",
    "# Compare with sklearn's LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "sk_model = LinearRegression()\n",
    "sk_model.fit(X_train, y_train)\n",
    "sk_score = sk_model.score(X_test, y_test)\n",
    "print(f\"sklearn LinearRegression R-squared score: {sk_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 3**\n",
    "\n",
    "In this exercise, you will compare the performance of your custom MultipleLinearRegression implementation with sklearn's LinearRegression. You'll examine the coefficients, intercept, and performance metrics while adjusting the learning rate of your custom implementation.\n",
    "\n",
    "#### Task:\n",
    "\n",
    "1. Generate a synthetic dataset using sklearn's `make_regression` function.\n",
    "2. Implement a function to train both your custom MultipleLinearRegression and sklearn's LinearRegression on the same data.\n",
    "3. Compare the coefficients, intercept, and R-squared scores of both models.\n",
    "4. Experiment with different learning rates for your custom model and observe how it affects the results.\n",
    "5. Create a plot showing the R-squared scores of your custom model for different learning rates.\n",
    "\n",
    "#### Requirements:\n",
    "\n",
    "- Use the MultipleLinearRegression class you implemented in Exercise 5.\n",
    "- Test at least 5 different learning rates for your custom model.\n",
    "- Create a plot comparing the performance (R-squared scores) of your custom model with different learning rates to sklearn's LinearRegression.\n",
    "- Print a comparison of coefficients and intercepts for the best performing custom model and sklearn's model.\n",
    "\n",
    "#### Hints:\n",
    "\n",
    "- You can use `numpy.logspace` to generate a range of learning rates to test.\n",
    "- Consider using a validation set to select the best learning rate for your custom model.\n",
    "- Remember to reset your custom model before training with each new learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Assuming you have your MultipleLinearRegression class from Exercise 5\n",
    "# from multiple_linear_regression import MultipleLinearRegression\n",
    "\n",
    "def compare_models(X_train, X_test, y_train, y_test, learning_rates):\n",
    "    ## Implement me\n",
    "    pass\n",
    "    # Should return sk_model, sk_score, custom_scores\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=1000, n_features=5, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define learning rates to test\n",
    "learning_rates = np.logspace(-4, 0, 9)\n",
    "\n",
    "# Compare models\n",
    "sk_model, sk_score, custom_scores = compare_models(X_train, X_test, y_train, y_test, learning_rates)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(learning_rates, custom_scores, 'bo-', label='Custom Model')\n",
    "plt.axhline(y=sk_score, color='r', linestyle='--', label='sklearn Model')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('R-squared Score')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find best performing custom model\n",
    "best_lr_index = np.argmax(custom_scores)\n",
    "best_lr = learning_rates[best_lr_index]\n",
    "print(f\"Best learning rate for custom model: {best_lr:.6f}\")\n",
    "\n",
    "# Train best custom model\n",
    "best_custom_model = MultipleLinearRegression(learning_rate=best_lr, n_iterations=1000)\n",
    "best_custom_model.fit(X_train, y_train)\n",
    "\n",
    "# Compare coefficients and intercepts\n",
    "print(\"\\nCoefficients comparison:\")\n",
    "print(\"sklearn Model:\", sk_model.coef_)\n",
    "print(\"Custom Model:\", best_custom_model.coef_)\n",
    "\n",
    "print(\"\\nIntercept comparison:\")\n",
    "print(\"sklearn Model:\", sk_model.intercept_)\n",
    "print(\"Custom Model:\", best_custom_model.intercept_)\n",
    "\n",
    "print(\"\\nR-squared scores:\")\n",
    "print(\"sklearn Model:\", sk_score)\n",
    "print(\"Best Custom Model:\", custom_scores[best_lr_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will learn about scikit-learn pipelines and use them to combine your custom LogTransform class from Exercise 2 from last week with your MultipleLinearRegression class from Exercise 2. This will demonstrate how your custom classes can be integrated into the scikit-learn ecosystem.\n",
    "\n",
    "#### Introduction to Pipelines\n",
    "\n",
    "Pipelines in scikit-learn are a way to chain multiple steps that can be cross-validated together while setting different parameters. They help in preventing data leakage between train and test sets and make your code cleaner and more modular.\n",
    "\n",
    "The `make_pipeline` function is a simple way to create a pipeline. It takes a series of estimators and returns a pipeline that chains them in sequence.\n",
    "\n",
    "#### Task:\n",
    "\n",
    "1. Create a pipeline that applies the LogTransform to the input features and then uses the MultipleLinearRegression for prediction.\n",
    "2. Compare the performance of this pipeline with a pipeline using sklearn's StandardScaler and LinearRegression.\n",
    "3. Use the pipelines on a dataset where a log transformation might be beneficial (e.g., data with exponential relationships in the features).\n",
    "\n",
    "#### Requirements:\n",
    "\n",
    "- Use the LogTransform class you implemented in Exercise 2.\n",
    "- Use the MultipleLinearRegression class you implemented in Exercise 5.\n",
    "- Create two pipelines using `make_pipeline`:\n",
    "  a. Custom pipeline: LogTransform -> MultipleLinearRegression\n",
    "  b. sklearn pipeline: StandardScaler -> LinearRegression\n",
    "- Generate a synthetic dataset where log transformation of features could be beneficial.\n",
    "- Compare the R-squared scores of both pipelines.\n",
    "- Create scatter plots comparing the predictions of both pipelines against the true values.\n",
    "\n",
    "#### Hints:\n",
    "\n",
    "- You can use `make_regression` with an exponential transformation on the features to create a dataset where log transformation might be useful.\n",
    "- Remember to handle any potential issues with non-positive values when applying the log transform.\n",
    "- Use scikit-learn's `make_pipeline` function to create the pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Assuming you have your LogTransform and MultipleLinearRegression classes from previous exercises\n",
    "# from log_transform import LogTransform\n",
    "# from multiple_linear_regression import MultipleLinearRegression\n",
    "\n",
    "# Generate synthetic data with exponential relationship in features\n",
    "n_samples, n_features = 1000, 5\n",
    "X, y = make_regression(n_samples=n_samples, n_features=n_features, noise=0.1, random_state=42)\n",
    "X = np.exp(X)  # Apply exponential transformation to create non-linear relationship in features\n",
    "X = np.abs(X) + 1e-5  # Ensure all values are positive for log transform\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create custom pipeline\n",
    "custom_pipeline = make_pipeline(\n",
    "    #implment me\n",
    ")\n",
    "\n",
    "# Create sklearn pipeline\n",
    "sklearn_pipeline = make_pipeline(\n",
    "    #implment me\n",
    ")\n",
    "\n",
    "# Fit both pipelines\n",
    "custom_pipeline.fit(X_train, y_train)\n",
    "sklearn_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "custom_pred = custom_pipeline.predict(X_test)\n",
    "sklearn_pred = sklearn_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate R-squared scores\n",
    "custom_r2 = r2_score(y_test, custom_pred)\n",
    "sklearn_r2 = r2_score(y_test, sklearn_pred)\n",
    "\n",
    "print(\"R-squared scores:\")\n",
    "print(f\"Custom Pipeline: {custom_r2:.4f}\")\n",
    "print(f\"sklearn Pipeline: {sklearn_r2:.4f}\")\n",
    "\n",
    "# Create scatter plots\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(y_test, custom_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Custom Pipeline: LogTransform + MultipleLinearRegression\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(y_test, sklearn_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"sklearn Pipeline: StandardScaler + LinearRegression\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare pipeline steps\n",
    "print(\"\\nCustom Pipeline steps:\")\n",
    "print(custom_pipeline.steps)\n",
    "\n",
    "print(\"\\nsklearn Pipeline steps:\")\n",
    "print(sklearn_pipeline.steps)\n",
    "\n",
    "# If you want to access the coefficients of the regression models:\n",
    "custom_coef = custom_pipeline.named_steps['multiplelinearregression'].coef_\n",
    "sklearn_coef = sklearn_pipeline.named_steps['linearregression'].coef_\n",
    "\n",
    "print(\"\\nRegression Coefficients:\")\n",
    "print(\"Custom Pipeline:\", custom_coef)\n",
    "print(\"sklearn Pipeline:\", sklearn_coef)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
